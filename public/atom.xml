<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>吴卫东のBlog</title>
  
  <subtitle>我究竟拿时间换了什么</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://wvdon.com/"/>
  <updated>2021-01-28T13:30:41.451Z</updated>
  <id>http://wvdon.com/</id>
  
  <author>
    <name>Wedong Wu</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>论文阅读笔记-推荐系统类 </title>
    <link href="http://wvdon.com/2021/01/28/paper/P7-recommendSystem/"/>
    <id>http://wvdon.com/2021/01/28/paper/P7-recommendSystem/</id>
    <published>2021-01-28T13:30:00.000Z</published>
    <updated>2021-01-28T13:30:41.451Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Modelling-High-Order-Social-Relations-for-Item-Recommendation"><a href="#Modelling-High-Order-Social-Relations-for-Item-Recommendation" class="headerlink" title="Modelling High-Order Social Relations for Item Recommendation"></a>Modelling High-Order Social Relations for Item Recommendation</h2><blockquote><p>来源谷歌学术</p><p>题目：为项目推荐推荐建立高阶联系</p></blockquote><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>这篇文章研究了社会关系对于推荐的影响，以往的方法都是使用一阶的社会关系（相当于P2P的网络结构，但是距离为1），而很少去使用高阶关系。</p><a id="more"></a><p><strong>社会关系在推荐系统中的重要性：</strong> 人们在真实的场景中，计划去买东西的时候，他的决定往往会受到朋友的影响。比如向朋友征求意见，或者被朋友所购买的产品所吸引。再随着社交网络，朋友圈，微信群这些的诞生，人们购买商品的交流变得更加方便，因此想要提供令人满意的服务，就不得不考虑社会关系带来的影响了。</p><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p>之前常见的解决方法是设计额外的损失项，将社会影响假设进行编码，最后和推荐目标一起优化。但是这些方法在利用社交关系上过于含蓄，因为他需要仔细的调整社会正规化系数才能得到好的结果。</p><p>基于以上，作者提出了一种新的基于GCN的预测模型，该模型通过多步消息传播将高阶社会关系明确编码到用户嵌入学习中。</p><p>然后在豆瓣和yelp的数据集上进行试验。并且与BPR,NCF,NSCR等模型进行对比，HOSE在任何情况下都能够达到最好的性能</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>在这项工作中，作者提出了一个新的社会推荐框架HOSR，该框架整合了高阶邻居信息来解决数据稀疏问题。该模型的核心是通过沿高阶社会邻居进行嵌入传播来生成用户嵌入。利用图卷积层，作者可以将高阶邻域的效果显式建模到表示框架中。进一步利用注意机制来杠杆不同层次的输出，并采用两种dropout策略来缓解过拟合。在两个真实数据集上的实验证明了他的模型的有效性和传播的影响。</p>]]></content>
    
    <summary type="html">
    
      每日论文翻译
    
    </summary>
    
      <category term="paper" scheme="http://wvdon.com/categories/paper/"/>
    
    
  </entry>
  
  <entry>
    <title>Cloud Computing and Its Technologies: A Survey </title>
    <link href="http://wvdon.com/2021/01/27/paper/P6-cloud-computing/"/>
    <id>http://wvdon.com/2021/01/27/paper/P6-cloud-computing/</id>
    <published>2021-01-27T13:27:25.000Z</published>
    <updated>2021-01-27T13:27:26.551Z</updated>
    
    <content type="html"><![CDATA[<ul><li>云计算技术综述</li><li>来源 百度学术</li><li>Mohd Saleem </li></ul><p>Abstract: Cloud computing play a big role in today business environment. It provides on-demand services. Here we  provide what actually cloud computing is and it’s various computing technologies and their characteristics. </p><a id="more"></a><p>Introduction-Cloud computing means using resources remotely rather than locally. Today cloud computing play an important role in  business applications. Cloud computing remove the headaches of managing hardware and software for the users, it is  the responsibility of cloud provider for managing all these. Cloud computing is an internet based computing where  different services are provided to customers with the help of internet. Fig 1 shows a sample cloud computing  architecture。The two most significant component of cloud architecture is the front end and the back end [1]. The front end is the  computer user and the application that is used to access the cloud via a user interface such as web browser. The back  end is the cloud itself that include servers and cloud storage devices. </p><p>翻译：</p><p>云计算在今天的商业环境中很重要。他为我们听力提供了 基础性服务。在这篇文章我们会说明什么是云计算和他不同的科技和特征。</p><p>介绍:</p><p>云计算意味着不是使用本地而是使用远程资源。今天云计算对于商业应用非常重要。云计算为用户消除了令人头疼的管理硬件和软件。对于管理这些是云计算提供者的责任。<u>云计算是基于不同服务商在互联网的帮助下提供给消费者的基于计算的网络。</u><strong>云计算是一种基于互联网的计算，通过互联网为客户提供不同的服务</strong>。图1 是云计算结构的图例。云计算结构中最重要的两个组间是前端和后端。前端是是计算机用户和被用户使用去获取用户交互的应用，比如网络浏览器。后端是他自己，包括云计算服务和存储设备。</p><ul><li>component 组间</li><li>the front end and the back end  前端和后端</li></ul>]]></content>
    
    <summary type="html">
    
      每日论文翻译
    
    </summary>
    
      <category term="paper" scheme="http://wvdon.com/categories/paper/"/>
    
    
  </entry>
  
  <entry>
    <title>Reliability in Cloud Computing System: A Review</title>
    <link href="http://wvdon.com/2021/01/23/paper/P5-cloud-computing/"/>
    <id>http://wvdon.com/2021/01/23/paper/P5-cloud-computing/</id>
    <published>2021-01-23T14:24:25.000Z</published>
    <updated>2021-01-23T14:34:33.047Z</updated>
    
    <content type="html"><![CDATA[<ul><li>云计算可靠性研究综述</li><li>来源 计算机研究与发展  <a href="http://crad.ict.ac.cn/CN/1000-1239/home.shtml" target="_blank" rel="noopener">http://crad.ict.ac.cn/CN/1000-1239/home.shtml</a></li><li>华东师范，段等人。</li></ul><p>Abstract— As a new computing paradigm, cloud computing has attracts extensive concerns from both academic and industrial fields. Based on resource virtualization technology, cloud computing provides users with services in the forms of infrastructure, platform and software in a “pay-as-you-go” manner. In the meanwhile, since cloud computing provides highly scalable computing resources, more and more enterprises and organizations choose cloud computing platforms to deploy their scientific or commercial applications. However, with the increasing number of cloud users, cloud data centers continuously expand and the architecture becomes increasingly complex, leading to growing runtime failures in cloud computing systems. Therefore, how to ensure the system reliability in cloud computing systems with large scale and complex architecture has become a huge challenge. This paper first summarizes various failures in cloud systems, introduces several methods to evaluate the reliability of cloud computing, and describes some key fault management mechanisms. Since fault management techniques inevitably increase energy consumption of cloud systems, this paper reviews current researches on the trade-off between reliability and energy efficiency in cloud computing. In the end, we propose some major challenges in current research of cloud computing reliability and concludes our paper. </p><a id="more"></a><p> <strong>Key words:</strong> <a href="http://crad.ict.ac.cn/CN/10.7544/issn1000-1239.2020.20180675#" target="_blank" rel="noopener">cloud computing, </a><a href="http://crad.ict.ac.cn/CN/10.7544/issn1000-1239.2020.20180675#" target="_blank" rel="noopener">virtualization, </a><a href="http://crad.ict.ac.cn/CN/10.7544/issn1000-1239.2020.20180675#" target="_blank" rel="noopener">reliability, </a><a href="http://crad.ict.ac.cn/CN/10.7544/issn1000-1239.2020.20180675#" target="_blank" rel="noopener">fault management, </a><a href="http://crad.ict.ac.cn/CN/10.7544/issn1000-1239.2020.20180675#" target="_blank" rel="noopener">energy consumption</a> </p><p>翻译：</p><p>作为一种新型计算模式，云计算已经从学术和工业领域吸引大量的关注。基于资源虚拟化技术，云计算以一种按需使用的方式提供给用户基础设施平台和软件等服务。与此同时，自从云计算提供大规模计算资源，越来越多企业和组织机构选择云计算平台to部署他们的科技或者商业应用。然而，随着大量云用户的增长，云数据中心持续在扩张，架构变得日益复杂，这就导致了在云计算系统故障增加。因此，如何确保云计算系统在在大规模和复杂的云计算系统稳定变成了一个巨大的挑战。这篇文章第一次总结了在云系统中的各种失败。介绍了一些去评估云计算可靠性的方法，并且描述了一些关键的故障管理机制。由于故障管理技术会增加云系统的资源消耗，这篇文章介绍了在云计算中可靠性与能源权衡问题的研究现状。我们在当前云计算可靠性研究中列举了一些主要的挑战，并且总结归类在这篇文章中。（最后列举了当前云计算可靠性研究中存在的主要挑战）</p><p><strong>关键词</strong>; 云计算，虚拟化，可靠性，故障管理，能源消耗</p><ul><li>paradigm 模式</li><li>resource virtualization technology资源虚拟化技术</li><li>infrastructure  基础设施</li><li>runtime failures 运行故障</li><li>some key fault management <strong>mechanisms</strong>  一些关键的故障管理机制</li><li>reviews 分析，介绍 </li></ul>]]></content>
    
    <summary type="html">
    
      每日论文翻译
    
    </summary>
    
      <category term="paper" scheme="http://wvdon.com/categories/paper/"/>
    
    
  </entry>
  
  <entry>
    <title>使用frp远程访问Jupyter Notebook</title>
    <link href="http://wvdon.com/2021/01/21/linux/frp/"/>
    <id>http://wvdon.com/2021/01/21/linux/frp/</id>
    <published>2021-01-21T14:39:41.000Z</published>
    <updated>2021-01-21T14:39:21.195Z</updated>
    
    <content type="html"><![CDATA[<h2 id="frp穿透"><a href="#frp穿透" class="headerlink" title="frp穿透"></a>frp穿透</h2><p><a href="https://github.com/fatedier/frp/blob/master/README_zh.md" target="_blank" rel="noopener">软件下载&amp;官方文档</a> </p><p>可以通过下面的链接直接下载。</p><p><a href="http://web.wvdon.com/frp_0.29.0_linux_amd64.tar.gz" target="_blank" rel="noopener">下载链接</a></p><h3 id="配置本地与服务端："><a href="#配置本地与服务端：" class="headerlink" title="配置本地与服务端："></a>配置本地与服务端：</h3><ol><li>下载解压  </li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -xzvf  frp_0.29.0_linux_amd64.tar.gz`</span><br></pre></td></tr></table></figure><ol start="2"><li>服务器配置：frpc.ini</li></ol><a id="more"></a><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[common]</span><br><span class="line">bind_port = 7000</span><br><span class="line">vhost_http_port = 8080</span><br></pre></td></tr></table></figure><p>启动 </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup ./frps -c./frps.ini&amp;</span><br></pre></td></tr></table></figure><ol start="3"><li>客户端配置 frps.ini</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[common]</span><br><span class="line">server_addr = 111.111.110.211 #你的服务器ip</span><br><span class="line">server_port = 7000</span><br><span class="line"></span><br><span class="line">[web]</span><br><span class="line">type = http</span><br><span class="line">local_ip = 127.0.0.1</span><br><span class="line">local_port = 8888 #要映射的jupyter端口</span><br><span class="line">custom_domains = xxx.xxx.com # 映射到访问的web ,不加www 。另外需要提前将域名解析到服务器ip</span><br><span class="line">nohup ./frpc -c./frpc.ini&amp;</span><br></pre></td></tr></table></figure><ol start="4"><li>后台启动 jupyter notebook </li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup jupyter notebook &amp;</span><br></pre></td></tr></table></figure><p>使用cat查看是否jupyter启动端口和映射端口一致</p><p><code>cat nohup.out</code> 一致就不需要再管了，如果不一致可以使用 ps -aux找到该进程，然后kill -9 id 杀死，重新配置然后再启动。</p><p>此时可以通过web:8080端口访问穿透的Jupyter了。</p><h3 id="解决403-问题"><a href="#解决403-问题" class="headerlink" title="解决403 问题"></a>解决403 问题</h3><p>如果遇到远程访问403，说明本地是不允许访问的。</p><p>可以通过以下方法解决403 不允许访问。</p><ol><li>进到python环境里面先设置密码</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from notebook.auth import passwd</span><br><span class="line">passwd()#设置自己的密码，例如123</span><br><span class="line">#然后两次输入确认生成加密字符串</span><br></pre></td></tr></table></figure><p><img src="https://cdn.nlark.com/yuque/0/2021/png/646855/1611238111397-1bac0fe1-11ec-42f6-8b77-a127cd7917c0.png" alt="image.png"></p><p>记住上面产生的密码</p><ol start="2"><li>进行配置允许访问</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">jupyter notebook --generate-config</span><br><span class="line">vim ~/.jupyter/jupyter_notebook_config.py</span><br><span class="line">c.NotebookApp.allow_remote_access = True  #允许远程访问</span><br><span class="line">c.NotebookApp.allow_root = True          #允许root访问</span><br><span class="line">c.NotebookApp.ip=&apos;*&apos;                     # 所有ip皆可访问  </span><br><span class="line">c.NotebookApp.password = &apos;上面复制的那个字符串&apos;&apos;    </span><br><span class="line">c.NotebookApp.open_browser = False       # 禁止自动打开浏览器  </span><br><span class="line">c.NotebookApp.port =8888                 # 端口</span><br><span class="line">c.NotebookApp.notebook_dir = &apos;设置Notebook启动进入的目录&apos;</span><br></pre></td></tr></table></figure><ol start="3"><li>最后后台启动jupyter notebook</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup jupyter notebook &amp;</span><br></pre></td></tr></table></figure><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="https://blog.csdn.net/lawrencelue/article/details/107848856" target="_blank" rel="noopener">参考链接</a>:</p><p><a href="https://github.com/fatedier/frp/tree/master" target="_blank" rel="noopener">官网文档解读链接</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;frp穿透&quot;&gt;&lt;a href=&quot;#frp穿透&quot; class=&quot;headerlink&quot; title=&quot;frp穿透&quot;&gt;&lt;/a&gt;frp穿透&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/fatedier/frp/blob/master/README_zh.md&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;软件下载&amp;amp;官方文档&lt;/a&gt; &lt;/p&gt;&lt;p&gt;可以通过下面的链接直接下载。&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://web.wvdon.com/frp_0.29.0_linux_amd64.tar.gz&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;下载链接&lt;/a&gt;&lt;/p&gt;&lt;h3 id=&quot;配置本地与服务端：&quot;&gt;&lt;a href=&quot;#配置本地与服务端：&quot; class=&quot;headerlink&quot; title=&quot;配置本地与服务端：&quot;&gt;&lt;/a&gt;配置本地与服务端：&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;下载解压  &lt;/li&gt;
&lt;/ol&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;tar -xzvf  frp_0.29.0_linux_amd64.tar.gz`&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;ol start=&quot;2&quot;&gt;
&lt;li&gt;服务器配置：frpc.ini&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="tools" scheme="http://wvdon.com/categories/tools/"/>
    
    
      <category term="frp" scheme="http://wvdon.com/tags/frp/"/>
    
  </entry>
  
  <entry>
    <title>Microscaler: Automatic Scaling for Microservices with an Online Learning Approach</title>
    <link href="http://wvdon.com/2021/01/19/paper/P4-Microscaler/"/>
    <id>http://wvdon.com/2021/01/19/paper/P4-Microscaler/</id>
    <published>2021-01-19T06:24:25.000Z</published>
    <updated>2021-01-19T06:25:53.879Z</updated>
    
    <content type="html"><![CDATA[<ul><li>Microscaler ：使用在线学习方法对微服务进行自动度量</li><li>使用在线学习和启发式方法，去实现最优的服务规模。以满足负载均衡</li><li>余广坝,徐鹏飞,郑子彬 etc.</li><li>来源  <a href="https://ieeexplore.ieee.org/" target="_blank" rel="noopener">https://ieeexplore.ieee.org/</a> .</li><li>使用doi 下载链接  <a href="https://sci-hub.se/10.1109/ICWS.2019.00023" target="_blank" rel="noopener">https://sci-hub.se/10.1109/ICWS.2019.00023</a></li></ul><a id="more"></a><p>Abstract—<em>Recently, the microservice becomes a popular architecture to construct cloud native systems due to its agility. In</em></p><p><em>cloud native systems, autoscaling is a core enabling technique to adapt to workload changes by scaling out/in. However, it becomes a challenging problem in a microservice system, since such a system usually comprises a large number of different micro services with complex interactions. When bursty and unpredictable workloads arrive, it is difficult to pinpoint the scaling-needed services which need to scale and evaluate how much resource they need. In this paper, we present a novel system named Microscaler to automatically identify the scaling-needed services and scale them to meet the service level agreement (SLA) with an optimal cost for micro-service systems. Microscaler collects the quality of service metrics (QoS) with the help of the service mesh enabled infrastructure. Then, it determines the under-provisioning or over-provisioning services with a novel criterion named service power. By combining an online learning approach and a step by step heuristic approach, Microscaler could achieve the optimal service scale satisfying the SLA requirements. The experimental evaluations in a micro-service benchmark show that Microscaler converges to the optimal service scale faster than several state of the art methods.</em></p><p>翻译：</p><p>摘要：最近，由于微服务的敏捷性，他变成了构成本地云系统的一种流行结构。在本地云系统，自动度量是一种通过度量进出来调整工作负载机会的核心技术。然而，自从系统通常使用复杂的交互在不同的大量微服务中以来，微服务系统中他变成了一个挑战。当突发和不可预测的工作负载到达，那是很难去指出微服务需要多少度量并且评估他们需要多少资源。在这篇文章中，我们呈现了一个新系统，并将其命名为Mircroscaler，去自动识别这个服务度量需要，并且对于微服务系统使用一个优化消费去衡量他们，以满足服务水平协议。Microscaler 在启用服务网格的基础架构的帮助下收集了服务质量指标。然后，他使用名为service power的新标准去确定支配不足或过度支配服务。通过联合在线学习的方法和步步为营的启发式方法，Microscaler 可以实现这个最优服务规模，满足SLA的要求。在微服务基准中，实验评估表明:Microscaler收敛到最近服务规模的速度比先进最先进的方法都要快。</p><ul><li>agility 敏捷性</li><li>comprises 包括</li><li>bursty 突发</li><li>optimal 优化</li><li>the service level agreement  服务水平协议</li><li>the quality of service metrics (QoS)   服务质量指标</li><li><strong>the service mesh enabled infrastructure</strong>  <strong>启用服务网格的基础架构</strong></li><li>heuristic 启发式</li><li>under-provisioning 预配不足</li><li>criterion  标准</li><li>benchmark  基准</li><li>state of the art 最先进的</li><li>converges 收敛</li></ul>]]></content>
    
    <summary type="html">
    
      每日论文翻译
    
    </summary>
    
      <category term="paper" scheme="http://wvdon.com/categories/paper/"/>
    
    
  </entry>
  
  <entry>
    <title>Hdconfigor: Automatically Tuning High Dimensional Configuration Parameters for Log Search Engines</title>
    <link href="http://wvdon.com/2021/01/18/paper/P3-hdconfigor/"/>
    <id>http://wvdon.com/2021/01/18/paper/P3-hdconfigor/</id>
    <published>2021-01-18T07:00:09.000Z</published>
    <updated>2021-01-18T07:03:20.745Z</updated>
    
    <content type="html"><![CDATA[<ul><li><p>Hdconfigor:自适应改变日志搜索引擎的高纬度配置参数</p></li><li><p>徐鹏飞，郑子彬教授等</p></li><li><p>文章来源 <a href="https://ieeexplore.ieee.org/" target="_blank" rel="noopener">https://ieeexplore.ieee.org/</a></p></li><li><p>论文<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9079492" target="_blank" rel="noopener">链接</a></p></li></ul><p>发表在 <strong>IEEE Access</strong></p><a id="more"></a><h3 id="What-is-this-paper-doing"><a href="#What-is-this-paper-doing" class="headerlink" title="What is this paper doing?"></a>What is this paper doing?</h3><p>  面对大规模分布式系统提出一种可以自适应改变日志搜索引擎配置参数的方法，并设计实现了HDconfigor工具。</p><h3 id="翻译："><a href="#翻译：" class="headerlink" title="翻译："></a>翻译：</h3><h4 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h4><p> <em>ABSTRACT Search engines are nowadays widely applied to store and analyze logs generated by <strong>largescale distributed systems</strong>. To adapt to various <strong>workload scenarios</strong>, log search engines such as Elasticsearch usually expose a large number of performance-related configuration parameters. As manual configuring is time consuming and labor intensive, automatically tuning configuration parameters to optimize performance has been an urgent need. However, it is challenging because: 1) Due to the complex implementation, the relationship between performance and configuration parameters is difficult to model and thus the objective function is actually a black box; 2) In addition to application parameters, JVM and kernel parameters are also closely related to the performance and together they construct a high dimensional configuration space; 3) To iteratively search for the best configuration, a tool is necessary to automatically deploy the newly generated configuration and launch tests to measure the corresponding performance. To address these challenges, this paper designs and implements HDConfigor, an automatic holistic configuration parameter tuning tool for log search engines. In order to solve the high dimensional optimization problem, we propose a <strong>modified Random EMbedding Bayesian Optimization algorithm (mREMBO)</strong> in HDConfigor which is a black-box approach. Instead of directly using a black-box optimization algorithm such as Bayesian optimization (BO), mREMBO first generates a lower dimensional embedded space through introducing a random embedding matrix and then performs BO in this embedded space. Therefore, HDConfigor is able to find a competitive configuration automatically and quickly. We evaluate HDConfigor in an Elasticsearch cluster with different workload scenarios. Experimental results show that compared with the default configuration, the best relative median indexing results achieved by mREMBO can reach 2.07×. In addition, under the same number of trials, mREMBO is able to find a configuration with at least a further 10.31% improvement in <strong>throughput</strong> compared to Random search, Simulated Annealing and BO.</em> </p><p> 翻译：现今，搜索引擎正广泛应用于存储和分析大规模分布式系统所产生的日志。为了能够适应不同工作负载场景，像Elasticsearch这样的搜索引擎通常会产生大量的相关配置参数，随着手动配置的时间消耗和工人密集，自动化调整配置参数去优化性能现在已经变成一个紧急的需求。然而，这是一个挑战，因为1：由于复杂的实现，在性能和配置参数直接的关系是很难去模拟，因此这个目标函数实际上是一个黑盒。第二点，五六千应用参数，JAVA虚拟环境和内核参数也后入参数紧密相关，并且共建一个高纬度配置空间。第三点。为了迭代搜索这个最好的配置，自动部署新生成的配置是需要一个工具，并且运行测试去测量相应的参数。为了去应对这些挑战，这篇文章设计并且实现了 HDConfigor，一个对于日志搜索引擎自动历史配置参数的工具。为了去解决高纬度优化问题，我们提出了一个改进的随机嵌入贝叶斯优化算法（mREMBO）在HDConfigor,这是一个黑盒方法。代替直接使用黑盒优化算法，比如贝叶斯优化，mREMBO首次生成了一个低维度参数嵌入空间，通过引进一个随机的嵌入矩阵，然后再嵌入空间执行贝叶斯优化。因此，HDconfigor是能够自动并且快速的发现一个有竞争力的配置。我们在 Elasticsearch 集群下使用不同的工作负载环境对其进行评估，实验结果表明，对比这些默认的配置，mREMBO可以实现最好的结果，达到2.07。另外，在相同参数的实验下，通过对比随机搜索和  Simulated Annealing and BO ，我们能够发现一个吞吐量可以至少提升10.31%的配置。 </p><ul><li><strong>largescale distributed systems 大型分布式系统。</strong></li><li>workload scenarios 工作负载场景。,</li><li>manual 手动</li><li>optimize performance 优化性能</li><li>objective function 目标函数</li><li>iteratively  迭代</li><li>corresponding 相应的</li><li>embedded 嵌入的  random embedding matrix 随机嵌入矩阵</li><li>modified Random EMbedding Bayesian Optimization algorithm (mREMBO)  改进的随机嵌入贝叶斯优化算法</li><li>performs 执行</li><li>cluster 集群</li><li>trials 实验</li><li>throughput 吞吐量</li></ul>]]></content>
    
    <summary type="html">
    
      每日论文翻译
    
    </summary>
    
      <category term="paper" scheme="http://wvdon.com/categories/paper/"/>
    
    
  </entry>
  
  <entry>
    <title>Blockchain challenges and opportunities :a survey</title>
    <link href="http://wvdon.com/2021/01/17/paper/P2-BlockchainSuvery/"/>
    <id>http://wvdon.com/2021/01/17/paper/P2-BlockchainSuvery/</id>
    <published>2021-01-17T14:15:11.000Z</published>
    <updated>2021-01-17T14:17:58.133Z</updated>
    
    <content type="html"><![CDATA[<h3 id="What-is-this-paper-doing"><a href="#What-is-this-paper-doing" class="headerlink" title="What is this paper doing?"></a>What is this paper doing?</h3><p>  A survey:关于区块链的挑战和机会的综述 </p><h3 id="翻译："><a href="#翻译：" class="headerlink" title="翻译："></a>翻译：</h3><h4 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h4><p><em>Abstract: Blockchain has numerous beneﬁts such as decentralisation, persistency, anonymity and auditability. There is a wide spectrum of blockchain applications ranging from cryptocurrency, ﬁnancial services, risk management, internet of things (IoT) to public and social services. Although a number of studies focus on using the blockchain technology in various application aspects, there is no comprehensive survey on the blockchain technology in both technological and application perspectives. To ﬁll this gap, we conduct a comprehensive surveyon the blockchain technology. In particular, this paper gives the blockchain taxonomy, introduces typical blockchain consensus algorithms, reviews blockchain applications and discusses technical challenges as well as recent advances in tackling the challenges. Moreover, this paper also points out the future directions in the blockchain technology</em></p><a id="more"></a><p>区块链现在有很多好处，比如去中心化，连续性，匿名和可审计性。区块链非常广泛的应用到加密货币，金融服务，风险管理，物联网loT到公共和社会服务。尽管大量的研究聚焦到使用区块链技术在各个方面。但是现今在科技和应用方面，仍然没有一个关于区块链技术很综合的调查。为了弥补这个空缺，我们对区块链技术进行了一个比较综合的调查。尤其是，这篇文章<del>对区块链进行分类</del><strong>给出了区块链分类法</strong>，介绍了典型的区块链共识算法，综述了区块链应用和讨论起科技挑战，以及提前应对挑战。此外，这篇文章也指出了区块链技术的未来方向。</p><p>range from ….   to </p><ul><li>decentralisation 去中心化。</li><li>persistency 连续性</li><li>auditability 可信性</li><li>cryptocurrency 加密货币</li><li>internet of things (IoT)</li><li>conduct 在这翻译为进行</li><li>blockchain taxonomy  区块链分类</li><li>consensus algorithms 共识算法</li><li>tackling the challenges 应对挑战。</li></ul>]]></content>
    
    <summary type="html">
    
      每日论文翻译
    
    </summary>
    
      <category term="paper" scheme="http://wvdon.com/categories/paper/"/>
    
    
  </entry>
  
  <entry>
    <title>SFNN: Semantic Features Fusion Neural Network for Multimodal Sentiment Analysis</title>
    <link href="http://wvdon.com/2021/01/16/paper/P1-SFNN/"/>
    <id>http://wvdon.com/2021/01/16/paper/P1-SFNN/</id>
    <published>2021-01-16T11:52:57.000Z</published>
    <updated>2021-01-21T07:17:07.309Z</updated>
    
    <content type="html"><![CDATA[<h3 id="What-is-this-paper-doing"><a href="#What-is-this-paper-doing" class="headerlink" title="What is this paper doing?"></a>What is this paper doing?</h3><p> sentiment in online reviews  在线情感分析 </p><h3 id="翻译："><a href="#翻译：" class="headerlink" title="翻译："></a>翻译：</h3><h4 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h4><p>在线情感分析是一个重要的任务，在线情感分析的影响是这些应用的基础，比如用户偏好模型，消费者行为监测和公共观点分析。在先前的研究中，情感分析任务主要依赖于文本内容，忽视了在评论上视觉信息的模型影响。这篇文章基于提出了一个神经网络SFNN。这个模型第一次使用CNN卷积神经网络和注意力机制去获取图片所表达的情感特征，同时，映射这个情感表达特征到语义特征级别。此时，视觉方面的语义特征与文本模型的语义特征进行联合。最终，评论的情感极性被图片实体层的情感联合特征有效的分析。基于语义级别的特征融合可以减少异构数据的不同。实验结果表明，在基准数据集上，我们的模型比现存的方法能够取得更好的表现。  </p><a id="more"></a><h3 id="Introduction-介绍"><a href="#Introduction-介绍" class="headerlink" title="Introduction 介绍"></a>Introduction 介绍</h3><p>随着互联网的快速发展，越来越多的人进入到网络世界，用户和网站之间的交互会变得越来越频繁，这就导致了大量文本信息的快速增长，据报道，90%的消费者会在买东西之前先读评论，88%的消费者相信评论和熟人的建议。公司为了推荐需要了解用户喜好，或者去跟踪消费者对营销和产品设计的看法。而这一关键就是情感分析。</p><p>​    目前，绝大多数的研究者对于永和的情感识别是基于文本的。使用基于情感字典的方法，机器学习规则，基于深度学习的对网民的文本进行情感分析.Kim等人在情感字典的帮助下，通过计算情感得分来判断文本的情感。Pan 等人 第一次使用机器学习算法应用到情感分类任务，使用不同的机器学习模型在电影评论上进行文本分析。Tai等人提出LSTM长短期记忆网络去辨别电影评论的情感，并且取得了好的结果。尽管基于文本的情感识别已经取得了巨大的成就，不过这也是很困难去识别哪些被网民仅仅通过文本，并且他们所表达出来的文本内容是多种含义。</p><p>随着多媒体领域的到来，相比之前仅仅通过文本表达情感分方式。网民通常表达他们的情哥哥通过带有图片的文本。网民的情感和图片文本有一个映射关系。综合考虑文本和图片的情感可以一定程度上解决其多义性。通过融合文本和图片情感特征对情感进行分类是一种多模式融合情感识别问题。多模态融合情感识别的核心是对不同模态的特征提取，并且融合提取的特征。目前，多模态融合情感分析的问题尚未引起国内外学者的广泛关注。Rosas使用词袋模型去表示文本特征，通过OpenEar提取音频特征，使用OkaoVision应用于面部表达特征。然后切分不同模型的三个特征向量到长向量，并将它们输入到支持向量机题识别多模态情感。Majumder等人提出了一个新颖的特征层融合方法，这个方法是分层的方式。联合文本特征，语音特征和面部特征。三种特征融合在一起并输入到深度学习模型。 从以上文献可以看出，当前的多模态融合情感识别研究中视觉模态的选择主要是基于面部表情，所采用的方法较为通用，提取图片的语义信息不好。然而，考虑到真正互联网中的舆论事件。面部表情很少出现在网民所发布的帖子中。大多数时候使用的是带有情感极性的图片。</p><p>对于以上现存方法的问题，这篇文章提出了一个基于多模态数据的模型SFNN.这篇文章主要的贡献如下：</p><ul><li>使用图像对模型结构进行排序，图片的语义特征是被转换成语义顺序向量。它相比单独的使用图像特征，可以更好地捕获图像的语义特征。在使用文本特征进行融合后，他可以做更好的情感判断。</li><li>使用BERT预训练模型进行提取评论文本特征，他相比传统的Text-CNN，LSTM,BiGRU 能够更好的捕获句子之间的关系。</li></ul><p>这篇文章的结构如下。第二段介绍了在相关领域其他学者的工作。在第三段对于模型介绍和推导。第四段和第五段分别给出实验结果，实验结论，未来工作的展望。</p><h3 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h3><p>在线评论情感识别涉及到识别网民在社交平台发布的文本的情感。网民情感是在线评论的一个重要的元素。对于识别也有重大的实际意义。<strong>鉴于没有针对文本数据进行感性标记的统一标准，</strong>Wu等人基于心理模型OCC构造了文本情感标注规则，结合CNNs模型在深度学习中识别网民的情感，取得了良好的效果。He等人利用词向量表示技术，针对微博中表情符号的特征，构建了表情符号情感空间的特征表示矩阵，形成了具有情感极性的情感词向量。Wu等人从网民在突发事件中的真实情绪出发，提出了将情感词向量与BiLSTM相结合的模型，对网民的负面情绪进行识别，并将其分为愤怒、悲伤和恐惧三类。这些方法只关注评论的文本信息进行情感识别，缺乏对评论中的图像信息的关注。</p><p>​    图像情感识别是一个新的研究领域。在图像情感识别任务中，研究者们做出了不懈的努力。图像情感识别技术主要可分为三类:基于低级视觉特征的方法,通过提取出低级视觉特征与人类情绪有关,如颜色和其他手工特性,结合相关知识,使用分类器分类图片的情绪。基于中间层语义表示的方法是通过构建中层语义特征来弥补底层特征与人的情感之间的语义鸿沟，形容词和名词对是中层语义表达的一种典型方法。基于深度学习的方法，利用深度神经网络构建深度模型，对图片的情感进行分类。深度学习在计算机视觉领域取得了重大突破。You等在迁移学习的基础上，在ImageNet上使用了预先训练好的CNNs模型，对300多万张不同情感的弱标记图像进行了微调，结果表明CNNs模型与机器学习相比具有很大的优势。</p><p>​    总的来说，现有的关于网络舆情中互联网用户情感识别的研究大多是基于文本，利用自然处理技术对文本情感进行识别。然而，随着多媒体时代的到来，网民情绪的表达方式发生了变化。与以往仅通过文字表达情感的方式相比，互联网用户更倾向于通过文字与图片相结合的方式来表达情感。为此，本文提出了一种基于多模态数据的情感分析模型。首先<strong>利用注意机制提取图像中的重要语义特征，然后利用递归神经网络将语义特征转化为不带标注的特殊图像语义向量。最后，利用注意机制对特征进行权重赋值，并利用图像语义向量对Bert提取的文本语义向量进行增强</strong>。实验表明，本文提出的模型在网络评论情感分析方面比现有模型有更好的效果。</p><h3 id="模型概述"><a href="#模型概述" class="headerlink" title="模型概述"></a>模型概述</h3><h4 id="模型介绍"><a href="#模型介绍" class="headerlink" title="模型介绍"></a>模型介绍</h4><p>​    用户评论通常包含文本和视觉内容，两者都提供重要的补充信息。因此，在网络评论的情感识别中，多模态检测方法比单模态检测方法具有更好的泛化性能。考虑到传统模型不能很好地提取图像部分的语义信息，我们提出了SFNN模型，如图1所示。</p><p>​    本文提出的模型集成了四个主要模块:视觉特征提取模块、文本特征提取模块、视觉语义向量提取模块和多模态特征融合模块。下面将详细介绍其模块的具体推导过程。</p><h4 id="视觉特征提取模块"><a href="#视觉特征提取模块" class="headerlink" title="视觉特征提取模块"></a>视觉特征提取模块</h4><p>为了获得视觉部分良好的语义表示，我们首先使用VGG预训练模型对输入图像进行编码。VGG卷积神经网络是对许多图像相关任务的图像表示学习有效。我们使用VGG16来获得图像的表示，通过模型输入得到最后一个全连接层的输出，然后再进入分类层。图像表示是一个从图像编码的4096维向量。</p><p>然后我们利用注意机制来接受图像的语义特征，目的是突出图像中具有强烈情感表达的部分，并且某些部分对情感分析更有意义。因此，当获得一个视觉模态表示时，每个特征将被赋予一个权值，表示其在视觉模态表示中的“重要性”。我们使用柔和的注意力。</p><h4 id="图像语义特征提取模块"><a href="#图像语义特征提取模块" class="headerlink" title="图像语义特征提取模块"></a>图像语义特征提取模块</h4><p>在视觉特征提取模块中，我们得到了图像的特征表达。为了获得更好的图像语义表达，我们使用一个BiGRU来形成图像序列模块。图像到序列模块通常用于图像标题，它用于将图像特征与文本特征对齐。我们将图像的特征表达式发送给BiGRU，得到图像的语义向量，这有助于表示图像的语义信息，如式(5)所示:</p><h4 id="文本特征提取模块"><a href="#文本特征提取模块" class="headerlink" title="文本特征提取模块"></a>文本特征提取模块</h4><p>传统的词向量模型对短句和无歧义句的模态分析有较好的效果。然而，在现实中处理过的句子并没有那么简单。要解决一词多义的问题，就要充分考虑词的前后关系。BERT模型是谷歌提出的一种新的语言表示模型。与传统的文本情感分析模型相比，该模型能够更好地覆盖上下文之间的联系。本文使用谷歌Research发布的预训练模型BERTBase来提取文本特征。对于输入文本，我们使用Bert进行特征提取，如式(6)所示:</p><p>为了更好地捕捉全局特征信息，更好地与图像语义信息融合，我们在提取BERT后，使用BiGRU进行特征提取，如式(7)所示:</p><h4 id="多模态特征融合模块"><a href="#多模态特征融合模块" class="headerlink" title="多模态特征融合模块"></a>多模态特征融合模块</h4><p>对于提取的图像特征和文本特征，我们提出了一种融合网络来拼接这些特征。具体来说，我们首先融合视觉模态的语义向量和文本模态h = [hv 1, hv 2，…]，hv i, ht 1, ht 2，…，然后将拼接好的特征发送到注意机制中。我们通过注意机制来突出那些有价值的特征，计算增强后的特征表示法，如式(8)~(10)所示:</p><p>然后我们将使用主成分分析(PCA)算法来融合特征e=[e1, e2, e3，…]， en]T以图像的原始特征i=[i1, i2, i3，…]，在]T中，PCA可以减少不同数据的差异。通过PCA算法，我们可以得到最终的特征向量f。最后，使用Softmax对最终融合的特征进行分类，如式(11)所示:</p><p>在本节中，我们将描述用于实验的数据集，并报告结果和必要的分析</p><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><h4 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h4><p>我们使用了来自Yelp.com的在线食物和餐馆评论数据集，覆盖了美国的五个主要城市:波士顿(BO)、芝加哥(CH)、洛杉矶(LA)、纽约(NY)和旧金山(SF)。洛杉矶是最大的，有最多的文件和图像。波士顿(BO)是最小的。然而，就句子数量和单词数量而言，这五个城市的文件长度非常相似。该数据集共有超过44,000条评论，其中包括244,000张图片。对于我们的实验，每个合成至少有3张图像。</p><h4 id="基线"><a href="#基线" class="headerlink" title="基线"></a>基线</h4><p>在本节中，我们比较了SFNN模型与之前提出的多模态情感分析模型的性能。我们比较以下基线:</p><ul><li><p>BiGRU - avgg和BiGRU - mvgg是连接BiGRU从文本学习的表示和VGG从图像学习的表示的组合，并将它们提供给一个分类层。</p></li><li><p>HAN-aVGG和HAN-mVGG是文字类的HAN-ATT(文字情感分析前沿)和图像类的VGG的结合。hanatt[17]使用单词和句子编码器来实现文档的层次结构。</p></li><li><p>VistaNet[18]提出,照片不独立表达文本的情绪,而是突出显示的文本的一个辅助部分的内容,它使用图片指导文本的关注,这决定了文档中的重要性程度不同的句子情感分类的文档。</p></li></ul><h4 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h4><p>从表I可以看出，与其他模型相比，我们的SFNN模型在Yelp上获得了最好的结果62.80%，比VistaNet模型提高了2.1%</p><p>​    TFN模型和HAN模型提供了丰富的文本特征和视觉特征之间的交互，但对数据的性能较差，在比较方法中准确率最低，分别为43.89%、46.87%、53.16%和55.01%。原因在于分析的直接融合功能的表达图片和文本的功能表达,和图像的语义信息不能得到,因为这些特性不携带相同的情绪推动信息,和他们保持更多的物理层的视觉模式。Truong提出的VistaNet模型并没有直接利用图像的特征，而是将图像转化为注意权重来突出文本中的情感信息。可见，VistaNet对数据集的准确率为61.88%。虽然我们提出的模型也文本特征的融合和图像特性,与之前的方法相比,我们的模型可以更好地提取图像语义信息,融合的特性在语义层面,并协助与图像的特性在物理层面,我们得到更好的实验结果。</p><h4 id="架构消融分析"><a href="#架构消融分析" class="headerlink" title="架构消融分析"></a>架构消融分析</h4><p>为了研究SFNN的每个组件的贡献，我们进行了消融分析，从最基本的配置开始，增加构建完整架构的组件。结果汇总于表二。</p><p>​    我们从文本特征提取模块开始，只依赖文本。从第一行可以看出，平均准确率达到58.84%。然后我们将文本特征与VGG16提取的图像特征直接拼接，可以看到效果提高了2.6%。在加入图像语义提取模块后，我们得到图像和文本在语义层面的特征。两者直接融合后，我们的平均识别准确率比之前提高了5.6%。最后，加入多模态特征融合模块，利用注意机制和主成分分析方法(PCA)对图像的语义级特征进行融合，并分别对图像的文字和物理层面特征进行了分析。最终，平均精度达到62.80%。实验结果表明，该模型的每个组成部分都有自己的贡献。</p><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>本文研究了多模态评论情感分析问题。针对现有模型大多直接拼接图像特征和文本特征，而不能获得图像的语义特征的问题，我们提出了一种新的多模态评论情感分析模型SFNN。具体来说，我们的模型集成了四个主要组件:视觉特征提取模块、文本特征提取模块、视觉语义向量提取模块和多模态特征融合模块。通过这四个模块的协同工作，我们可以在语义层面上融合图像语义向量和文本语义向量，得到更好的情感表达，同时结合图像的物理特征对评论进行分析。在美国5个主要城市的实验数据表明，我们的SFNN模型在情感分析方面优于多模态基线。该模型比现有模型具有更好的鲁棒性。</p>]]></content>
    
    <summary type="html">
    
      用于多模态情感分析的语义特征融合神经网络
    
    </summary>
    
      <category term="paper" scheme="http://wvdon.com/categories/paper/"/>
    
    
  </entry>
  
  <entry>
    <title>第四章 ：朴素贝叶斯</title>
    <link href="http://wvdon.com/2020/12/31/machineLearning/machineLearning/%E7%AC%AC%E5%9B%9B%E7%AB%A0/"/>
    <id>http://wvdon.com/2020/12/31/machineLearning/machineLearning/第四章/</id>
    <published>2020-12-31T13:04:52.000Z</published>
    <updated>2020-12-31T11:34:46.550Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>第三章 ：决策树</title>
    <link href="http://wvdon.com/2020/12/31/machineLearning/machineLearning/2020-12-31-%E7%AC%AC%E4%B8%89%E7%AB%A0-%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    <id>http://wvdon.com/2020/12/31/machineLearning/machineLearning/2020-12-31-第三章-：决策树/</id>
    <published>2020-12-31T13:04:52.000Z</published>
    <updated>2020-12-31T11:58:53.424Z</updated>
    
    <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>决策树算法类似于数据结构中的二分查找，可以使不同类型的数据集合，建立不同的分类器，最终可以通过决策树给出近似正确的结果。</p><h2 id="决策树-Decision-Tree"><a href="#决策树-Decision-Tree" class="headerlink" title="决策树 Decision Tree"></a>决策树 Decision Tree</h2><p>例如一个邮件系统，通过数个问题推断，不断进行缩小答案。</p><a id="more"></a><p> <img src="https://camo.githubusercontent.com/baee424187b6840aff667d476fa47d2f746f143fbd5ba8f5f78e39d71b1e81fe/687474703a2f2f646174612e617061636865636e2e6f72672f696d672f41694c6561726e696e672f6d6c2f332e4465636973696f6e547265652f2545352538362542332545372541442539362545362541302539312d2545362542352538312545372541382538422545352539422542452e6a7067" alt="决策树-流程图"> </p><h2 id="构建-The-process-of-building-a-Decision-Tree"><a href="#构建-The-process-of-building-a-Decision-Tree" class="headerlink" title="构建 The process of building a Decision Tree"></a>构建 The process of building a Decision Tree</h2><p>包括 特征选择、决策树的生成和决策树的修剪。 </p><p>算法：C4.5和CART</p><h2 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h2><p>相比KNN算法，可以使用不同的数据集合。</p><p>对中间值的缺失不敏感，可以处理不相关的特征值、</p><p>可能会产生过度匹配问题。容易过拟合。</p><p>未完。。。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要&quot;&gt;&lt;/a&gt;摘要&lt;/h2&gt;&lt;p&gt;决策树算法类似于数据结构中的二分查找，可以使不同类型的数据集合，建立不同的分类器，最终可以通过决策树给出近似正确的结果。&lt;/p&gt;&lt;h2 id=&quot;决策树-Decision-Tree&quot;&gt;&lt;a href=&quot;#决策树-Decision-Tree&quot; class=&quot;headerlink&quot; title=&quot;决策树 Decision Tree&quot;&gt;&lt;/a&gt;决策树 Decision Tree&lt;/h2&gt;&lt;p&gt;例如一个邮件系统，通过数个问题推断，不断进行缩小答案。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>先做好一件事，才能做好更多事</title>
    <link href="http://wvdon.com/2020/06/05/plan/%E5%85%88%E5%81%9A%E5%A5%BD%E4%B8%80%E4%BB%B6%E4%BA%8B%EF%BC%8C%E6%89%8D%E8%83%BD%E5%81%9A%E5%A5%BD%E6%9B%B4%E5%A4%9A%E4%BA%8B/"/>
    <id>http://wvdon.com/2020/06/05/plan/先做好一件事，才能做好更多事/</id>
    <published>2020-06-05T04:40:00.000Z</published>
    <updated>2021-01-23T14:37:14.159Z</updated>
    
    <content type="html"><![CDATA[<p><em>——用十年的沉淀，成为一名业界优秀的软件工程师</em></p><blockquote><p>ps :某次老师需要上交的工作职业规划</p></blockquote><p><strong>先引用之前与久哥的对话（百度T9架构师）：</strong></p><blockquote><p>他问，“如果让你用10年的时间学习数据库，你能不能成为这个领域的专家？”<br>我说，“应该可以吧”<br>他说，“你现在20，10年后也才30，30岁就能成为某个领域的专家，为什么不去做呢？你看看周围有多少30岁的人还一事无成，而那个时候的你已经是数据库的专家了。”</p></blockquote><a id="more"></a><h3 id="职业方向："><a href="#职业方向：" class="headerlink" title="职业方向："></a>职业方向：</h3><p><strong>软件开发工程师-数据挖掘/存储/计算/分布式方向</strong></p><h3 id="技术宗旨："><a href="#技术宗旨：" class="headerlink" title="技术宗旨："></a>技术宗旨：</h3><p>职业的发展并不只是需要技术栈的学习，当我进入到职场时需要面临的更多的是帮助公司如果实现盈利或拓展业务，而这则从我的职业思考中进行展开（如图）：<br><img src="https://img-blog.csdnimg.cn/20200605201735728.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1dlRG9uX3Q=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h4 id="知识："><a href="#知识：" class="headerlink" title="知识："></a>知识：</h4><p>知识是我能进入到职场进行生存的基石，比如我对 machine learning 和计算机相关知识和理论的储备</p><h4 id="工具："><a href="#工具：" class="headerlink" title="工具："></a>工具：</h4><p>人是会制造并且使用工具的高等动物，良好的工具能帮我高效率的解决当下问题。例如spark、Pytorch、xgboost等</p><h4 id="逻辑："><a href="#逻辑：" class="headerlink" title="逻辑："></a>逻辑：</h4><p>能够把握模型直接的逻辑性，并且提高解决问题和思维发散的能力。 </p><h4 id="业务："><a href="#业务：" class="headerlink" title="业务："></a>业务：</h4><p>业务是一个公司的现金流，可以让技术更好的切合业务模型。比如展示广告和搜索广告在构建模型时的区别联系，如何根据公司的 business model 制定模型的 objective</p><h3 id="十年规划"><a href="#十年规划" class="headerlink" title="十年规划"></a>十年规划</h3><h4 id="1-3年：¬¬"><a href="#1-3年：¬¬" class="headerlink" title="1-3年：¬¬"></a>1-3年：¬¬</h4><ol><li>技术上持续精进</li><li>提高学历</li><li>提高语言交流与阅读能力</li><li>提高自我认知</li><li>自律</li><li>断舍离<h4 id="3-7年："><a href="#3-7年：" class="headerlink" title="3-7年："></a>3-7年：</h4></li><li>技术深度</li><li>提高职场能力</li><li>把握系统架构</li><li>基金股票</li><li>注重积累<h4 id="7-10年："><a href="#7-10年：" class="headerlink" title="7-10年："></a>7-10年：</h4></li><li>技术广度¬¬</li><li>提高业务与管理能力</li><li>投资理财</li><li>身体健康</li></ol><p>==十年很短，你可不要太当真哦，所以我想用十年的沉淀，去成为一名业界优秀的软件工程师。==<br>                                                                                                                by wvdon  2020年06月05日</p>]]></content>
    
    <summary type="html">
    
      When I was just a little girl, I asked my mother, &quot;What will I be? Will I be pretty? Will I be rich?&quot; Here&#39;s what she said to me：&quot;Que sera, sera, Whatever will be, will be; The future&#39;s not ours to see.&quot;
    
    </summary>
    
      <category term="plan" scheme="http://wvdon.com/categories/plan/"/>
    
    
      <category term="work" scheme="http://wvdon.com/tags/work/"/>
    
  </entry>
  
  <entry>
    <title>数据挖掘上分思路</title>
    <link href="http://wvdon.com/2020/03/20/%E6%AF%94%E8%B5%9B%E6%80%9D%E8%B7%AF-%E9%98%85%E8%AF%BB/"/>
    <id>http://wvdon.com/2020/03/20/比赛思路-阅读/</id>
    <published>2020-03-20T07:50:39.000Z</published>
    <updated>2020-12-31T07:19:42.609Z</updated>
    
    <content type="html"><![CDATA[<p>比赛思路-阅读</p><p><a href="https://www.secrss.com/articles/15352" target="_blank" rel="noopener">https://www.secrss.com/articles/15352</a></p><p>BERT-Finetune、BERT-CNN-Pooling、BERT-RCNPooling多种结构进行融合!</p><p><img src="https://s.secrss.com/group1/M00/00/BC/Cvmo0l3YGuGAHWvrAAGOBh7iz9U059.jpg" alt></p><ul><li><p>每一个模型的基础上，进行10折交叉验证</p></li><li><p>利用 textrank4zh 对每条新闻文本取10个关键词，汇集所有的关键词，得到前100个出现最多的关键词。通过观察这些关键词，发现假新闻喜欢对部分人名、地名、名词、动词进行造谣。</p></li><li><p>模型融合+gru</p><p><img src="https://s.secrss.com/group1/M00/00/BC/Cvmo0l3YGuKAGCPJAAEdvEC-5_A736.png" alt></p></li></ul><a id="more"></a><p>假图片</p><p><strong>基本统计特征：</strong></p><ul><li><p>图片尺寸</p></li><li><p>图片后缀类型</p></li><li><p>图片模式（RGB、灰度等）</p></li><li><p>清晰度、亮度</p></li><li><p>直方图分布特征</p></li><li><p>各通道的均值方差等统计特征</p></li></ul><p><strong>特征意义：</strong></p><ul><li>关键特征包括图片尺寸和清晰度特征</li><li>图片尺寸可以识别图片的来源，比如手机截图的尺寸和相机照片尺寸截然不同</li></ul><p>一般认为图像越清晰越是真的，因为图像经过ps篡改之后清晰度会下降，还有一种可能性是谣言往往传播得更快，传播过程中的每一次保存和发送都可能会降低清晰度</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;比赛思路-阅读&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.secrss.com/articles/15352&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.secrss.com/articles/15352&lt;/a&gt;&lt;/p&gt;&lt;p&gt;BERT-Finetune、BERT-CNN-Pooling、BERT-RCNPooling多种结构进行融合!&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://s.secrss.com/group1/M00/00/BC/Cvmo0l3YGuGAHWvrAAGOBh7iz9U059.jpg&quot; alt&gt;&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;每一个模型的基础上，进行10折交叉验证&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;利用 textrank4zh 对每条新闻文本取10个关键词，汇集所有的关键词，得到前100个出现最多的关键词。通过观察这些关键词，发现假新闻喜欢对部分人名、地名、名词、动词进行造谣。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;模型融合+gru&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://s.secrss.com/group1/M00/00/BC/Cvmo0l3YGuKAGCPJAAEdvEC-5_A736.png&quot; alt&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="machineLearning" scheme="http://wvdon.com/categories/machineLearning/"/>
    
    
      <category term="数据处理" scheme="http://wvdon.com/tags/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/"/>
    
      <category term="比赛" scheme="http://wvdon.com/tags/%E6%AF%94%E8%B5%9B/"/>
    
  </entry>
  
  <entry>
    <title>1833：网易云里永远定格的数字</title>
    <link href="http://wvdon.com/2020/03/01/remenber/%E7%BA%AA%E5%BF%B5%E7%BD%91%E6%98%93%E4%BA%91%E9%87%8C%E9%82%A3%E4%BA%9B%E7%83%AD%E7%88%B1%E7%94%9F%E6%B4%BB%E7%9A%84%E4%BA%BA/"/>
    <id>http://wvdon.com/2020/03/01/remenber/纪念网易云里那些热爱生活的人/</id>
    <published>2020-03-01T07:50:39.000Z</published>
    <updated>2021-01-10T08:22:11.635Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1833：网易云里永远定格的数字"><a href="#1833：网易云里永远定格的数字" class="headerlink" title="1833：网易云里永远定格的数字"></a>1833：网易云里永远定格的数字</h2><p>​        2017年2月15日23时30分在上海市普陀区中山北路2605弄43号一处出租屋的发生一起火灾事故，此次事故导致4人死亡。<br>​        事后采访小区居民说，事发时一同在屋内的还有男主人的妻子和一对儿女。 “这个师傅人很好的，看到我们去他店里修东西，打招呼都很和善。女儿正在读大学，15日刚过完春节回来，儿子还在读小学，本来今天就应该开学了“。</p><a id="more"></a><p>😀<strong>我们虽不曾相识</strong>，也不知你走的那一刻有多孤单，害怕。可你什么都没有做错啊，为什么意外这样对待你啊。1833是这个小姐姐的听歌排行，她听的最多的一首歌是”<a href="https://music.163.com/#/song?id=32408774" target="_blank" rel="noopener"><strong>终有一天，我会抛弃你们</strong></a>“，而现在这个数字永远停留在了1833。我也多么希望这个数字哪怕会变一下，告诉我你还好。</p><blockquote><p>错的不是你，是这个世界。愿天堂有音乐，愿天堂没有火灾。</p></blockquote><hr><p><img src="https://img-blog.csdnimg.cn/20210110162123214.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1dlRG9uX3Q=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p>👩<a href="https://music.163.com/#/user/home?id=254967036" target="_blank" rel="noopener">@框女</a>  如今她也有了30885关注。</p><blockquote><p>或许我喜欢网易云的原因大概是每首歌的每一条评论里都包含了故事与爱。</p></blockquote><hr>]]></content>
    
    <summary type="html">
    
      纪念网易云里那些热爱生活的人，愿生活温柔以待你
    
    </summary>
    
      <category term="remember" scheme="http://wvdon.com/categories/remember/"/>
    
    
      <category term="网易云" scheme="http://wvdon.com/tags/%E7%BD%91%E6%98%93%E4%BA%91/"/>
    
      <category term="框女" scheme="http://wvdon.com/tags/%E6%A1%86%E5%A5%B3/"/>
    
  </entry>
  
  <entry>
    <title>文本特征处理</title>
    <link href="http://wvdon.com/2020/02/27/%E6%96%87%E6%9C%AC%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96/"/>
    <id>http://wvdon.com/2020/02/27/文本特征提取/</id>
    <published>2020-02-27T07:50:39.000Z</published>
    <updated>2021-01-10T08:27:40.123Z</updated>
    
    <content type="html"><![CDATA[<h1 id="文本特征提取"><a href="#文本特征提取" class="headerlink" title="文本特征提取"></a>文本特征提取</h1><p><a href="https://www.cnblogs.com/ljhdo/p/10578047.html" target="_blank" rel="noopener">参考</a></p><p>机器学习算法往往无法直接处理文本数据，需要把文本数据转换为数值型数据</p><h3 id="One-Hot"><a href="#One-Hot" class="headerlink" title="One-Hot"></a>One-Hot</h3><p>One-Hot方法很简单，但是它的问题也很明显：</p><ul><li>没有考虑单词之间的相对位置，任意两个词之间都是孤立的；</li><li>如果文档中有很多词，词向量会有很多列，但是只有一个列的值是1；</li></ul><a id="more"></a><p><strong>One-Hot表示的应用</strong></p><p>sklearn使用词袋（Bag of Words）和TF-IDF模型来表示文本数据，这两个模型都是One-Hot表示的应用，其中，词袋模型对应的就是文档向量。</p><h3 id="词袋模型"><a href="#词袋模型" class="headerlink" title="词袋模型"></a>词袋模型</h3><p>词袋模型（BoW）是用于文本表示的最简单的方法， BoW把文本转换为文档中单词出现次数的矩阵，该模型只关注文档中是否出现给定的单词和单词出现频率，而舍弃文本的结构、单词出现的顺序和位置。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">CountVectorizer(input=’content’, encoding=’utf<span class="number">-8</span>’, decode_error=’strict’, strip_accents=<span class="literal">None</span>, </span><br><span class="line">                lowercase=<span class="literal">True</span>,    preprocessor=<span class="literal">None</span>, tokenizer=<span class="literal">None</span>, stop_words=<span class="literal">None</span>, </span><br><span class="line">                token_pattern=’(?u)\b\w\w+\b’, ngram_range=(1, 1), analyzer=’word’, </span><br><span class="line">                max_df=<span class="number">1.0</span>, min_df=<span class="number">1</span>, max_features=<span class="literal">None</span>, vocabulary=<span class="literal">None</span>, binary=<span class="literal">False</span>, </span><br><span class="line">                dtype=&lt;<span class="class"><span class="keyword">class</span> ‘<span class="title">numpy</span>.<span class="title">int64</span>’&gt;)</span></span><br></pre></td></tr></table></figure><p>常用参数注释:</p><ul><li>input：默认值是content，表示输入的是顺序的字符文本</li><li>decode_error：默认为strict，遇到不能解码的字符将报UnicodeDecodeError错误，设为ignore将会忽略解码错误</li><li>lowercase：默认值是True，在分词（Tokenize）之前把文本中的所有字符转换为小写。</li><li>preprocessor：预处理器，在分词之前对文本进行预处理，默认值是None</li><li>tokenizer：分词器，把文本字符串拆分成各个单词（token），默认值是None</li><li>analyzer：用于预处理和分词，可设置为string类型，如’word’, ‘char’, ‘char_wb’，默认值是word</li><li>stop_words：停用词表，如果值是english，使用内置的英语停用词列表；如果是一个列表，那么使用该列表作为停用词，设为None且max_df∈[0.7, 1.0)将自动根据当前的语料库建立停用词表</li><li>ngram_range：tuple（min_n，max_n），表示ngram模型的范围</li><li>max_df：可以设置为范围在[0.0 1.0]的浮点数，也可以设置为没有范围限制的整数，默认为1.0。这个参数的作用是作为一个阈值，当构造语料库的词汇表时，如果某个词的document frequence大于max_df，这个词不会被当作关键词。如果这个参数是float，则表示词出现的次数与语料库文档数的百分比，如果是int，则表示词出现的次数。如果参数中已经给定了vocabulary，则这个参数无效</li><li>min_df：类似于max_df，不同之处在于如果某个词的document frequence小于min_df，则这个词不会被当作关键词</li><li>max_features：对所有关键词的term frequency进行降序排序，只取前max_features个作为关键词集</li><li>vocabulary：默认为None，自动从输入文档中构建关键词集，也可以是一个字典或可迭代对象。</li><li>binary：默认为False，一个关键词在一篇文档中可能出现n次；如果binary=True，非零的n将全部置为1，这对需要布尔值输入的离散概率模型的有用的</li><li><strong>dtype</strong> ：用于设置fit_transform() 或 transform()函数返回的矩阵元素的数据类型</li></ul><p>模型的属性和方法：</p><ul><li>vocabulary_：词汇表，字典类型</li><li>get_feature_names()：所有文本的词汇，列表型</li><li>stop_words_：停用词列表</li></ul><p>模型的主要方法：</p><ul><li>fit(raw_document)：拟合模型，对文本分词，并构建词汇表等</li><li>transform(raw_documents)：把文档转换为文档-词矩阵</li><li>fit_transform(raw_documents)：拟合文档，并返回该文档的文档-词矩阵</li></ul><h3 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a>TF-IDF</h3><p>TF-IDF模型用于对特征信息量进行缩放，当一个词在特定的文档中经常出现，而在其他文档中出现的频次很低，那么给予该词较高的权重；当一次词在多个文档中出现的频次都很高，那么给予该词较低的权重。如果一次单词在特定的文档中出现的频次很高，而在其他文档中出现的频次很低，那么这个单词很可能是该文档独有的词，能够很好地描述该文档。</p><p><strong>1，TF-IDF模型计算原理</strong></p><p>TF（ Term Frequency）是词频，表示每个单词在文档中的数量（频数），TF依赖于BoW模型的输出。</p><p>IDF（Inverse Document Frequency）是逆文档频率，代表一个单词的普遍成都，当一个词越普遍（即有大量文档包含这个词）时，其IDF值越低；反之，则IDF值越高。IDF是包含该单词的文档数量和文档总数的对数缩放比例</p><p><img src="https://img-blog.csdnimg.cn/20210110162720178.png#pic_center" alt="在这里插入图片描述"></p><p>TF-IDF（术语频率 - 逆文档频率）模型是TF和IDF相乘的结果：TF-IDF=TF*IDF。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">TfidfVectorizer(input=’content’, encoding=’utf<span class="number">-8</span>’, decode_error=’strict’, strip_accents=<span class="literal">None</span>, </span><br><span class="line">                lowercase=<span class="literal">True</span>,    preprocessor=<span class="literal">None</span>, tokenizer=<span class="literal">None</span>, stop_words=<span class="literal">None</span>, </span><br><span class="line">                token_pattern=’(?u)\b\w\w+\b’, ngram_range=(1, 1), analyzer=’word’, </span><br><span class="line">                max_df=<span class="number">1.0</span>, min_df=<span class="number">1</span>, max_features=<span class="literal">None</span>, vocabulary=<span class="literal">None</span>, binary=<span class="literal">False</span>, </span><br><span class="line">                dtype=dtype=&lt;class ‘numpy.float64’&gt;, </span><br><span class="line">                norm=’l2’, use_idf=<span class="literal">True</span>, smooth_idf=<span class="literal">True</span>, sublinear_tf=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><p>大部分参数和CountVectorizer相同，TfidfVectorizer独有的参数注释：</p><ul><li>norm=’l2’：每个输出行具备单位规范，当引用’l2’范式时，所有向量元素的平方和为1；当应用l2范数时，两个向量之间的余弦相似度是它们的点积。 *’l1’：向量元素的绝对值之和为1。</li><li>use_idf=True：启用IDF来重新加权</li><li>smooth_idf=True：平滑idf权重，向文档-词频矩阵的所有位置加1，就像存在一个额外的文档，只包含词汇表中的每个术语一次，目的是为了防止零分裂。</li><li>sublinear_tf=False：应用次线性tf缩放，默认值是False</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;文本特征提取&quot;&gt;&lt;a href=&quot;#文本特征提取&quot; class=&quot;headerlink&quot; title=&quot;文本特征提取&quot;&gt;&lt;/a&gt;文本特征提取&lt;/h1&gt;&lt;p&gt;&lt;a href=&quot;https://www.cnblogs.com/ljhdo/p/10578047.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;参考&lt;/a&gt;&lt;/p&gt;&lt;p&gt;机器学习算法往往无法直接处理文本数据，需要把文本数据转换为数值型数据&lt;/p&gt;&lt;h3 id=&quot;One-Hot&quot;&gt;&lt;a href=&quot;#One-Hot&quot; class=&quot;headerlink&quot; title=&quot;One-Hot&quot;&gt;&lt;/a&gt;One-Hot&lt;/h3&gt;&lt;p&gt;One-Hot方法很简单，但是它的问题也很明显：&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;没有考虑单词之间的相对位置，任意两个词之间都是孤立的；&lt;/li&gt;
&lt;li&gt;如果文档中有很多词，词向量会有很多列，但是只有一个列的值是1；&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="machineLearning" scheme="http://wvdon.com/categories/machineLearning/"/>
    
    
      <category term="nlp" scheme="http://wvdon.com/tags/nlp/"/>
    
      <category term="特征处理" scheme="http://wvdon.com/tags/%E7%89%B9%E5%BE%81%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>Sklearn-记录</title>
    <link href="http://wvdon.com/2019/12/27/machineLearning/sklearn%E5%AD%A6%E4%B9%A0%E4%BB%A3%E7%A0%81-%E4%BB%A5%E5%85%8D%E5%BF%98%E4%BA%86/"/>
    <id>http://wvdon.com/2019/12/27/machineLearning/sklearn学习代码-以免忘了/</id>
    <published>2019-12-27T12:12:51.000Z</published>
    <updated>2021-01-10T06:22:45.619Z</updated>
    
    <content type="html"><![CDATA[<h2 id="数据包"><a href="#数据包" class="headerlink" title="数据包"></a>数据包</h2><h3 id="sklearn-datasets"><a href="#sklearn-datasets" class="headerlink" title="sklearn datasets"></a>sklearn datasets</h3><p>提供一些导入，在线下载及本地生成数据集的方法。</p><blockquote><p>sklearn.datasets模块主要提供了一些导入、在线下载及本地生成数据集的方法，可以通过dir或help命令查看，我们会发现主要有三种形式：load_<dataset_name>、fetch_<dataset_name>及make_<dataset_name>的方法</dataset_name></dataset_name></dataset_name></p></blockquote><a id="more"></a><h3 id="train-test-split"><a href="#train-test-split" class="headerlink" title="train_test_split"></a>train_test_split</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_splitX_train,X_test,y_train,y_test = train_test_split(X,y,random_state = <span class="number">2003</span>)</span><br></pre></td></tr></table></figure><h3 id="线性模型"><a href="#线性模型" class="headerlink" title="线性模型"></a>线性模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets, linear_model <span class="comment"># 引用 sklearn库，主要为了使用其中的线性回归模块</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建数据集，把数据写入到numpy数组</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np  <span class="comment"># 引用numpy库，主要用来做科学计算</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt   <span class="comment"># 引用matplotlib库，主要用来画图</span></span><br><span class="line">data = np.array([[<span class="number">152</span>,<span class="number">51</span>],[<span class="number">156</span>,<span class="number">53</span>],[<span class="number">160</span>,<span class="number">54</span>],[<span class="number">164</span>,<span class="number">55</span>],</span><br><span class="line">                 [<span class="number">168</span>,<span class="number">57</span>],[<span class="number">172</span>,<span class="number">60</span>],[<span class="number">176</span>,<span class="number">62</span>],[<span class="number">180</span>,<span class="number">65</span>],</span><br><span class="line">                 [<span class="number">184</span>,<span class="number">69</span>],[<span class="number">188</span>,<span class="number">72</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印出数组的大小</span></span><br><span class="line">print(data.shape)</span><br><span class="line">x = data[:,<span class="number">0</span>].reshape(<span class="number">-1</span>,<span class="number">1</span>)</span><br><span class="line">y  = data[:,<span class="number">1</span>]</span><br><span class="line"><span class="comment"># TODO 1. 实例化一个线性回归的模型</span></span><br><span class="line">regr = linear_model.LinearRegression()</span><br><span class="line"><span class="comment"># TODO 2. 在x,y上训练一个线性回归模型。 如果训练顺利，则regr会存储训练完成之后的结果模型</span></span><br><span class="line"><span class="comment"># TODO 3. 画出身高与体重之间的关系</span></span><br><span class="line">regr.fit(x,y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 画出已训练</span></span><br><span class="line">plt.plot(x, regr.predict(x), color=<span class="string">'blue'</span>)<span class="comment"># 画x,y轴的标题</span></span><br><span class="line">plt.xlabel(<span class="string">'height (cm)'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'weight (kg)'</span>)</span><br><span class="line">plt.show() <span class="comment"># 展示</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 利用已经训练好的模型去预测身高为163的人的体重</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Standard weight for person with 163 is %.2f"</span>% regr.predict([[<span class="number">163</span>]]))</span><br></pre></td></tr></table></figure><h3 id="KNN"><a href="#KNN" class="headerlink" title="KNN"></a>KNN</h3><p>模型的泛化能力:它在新的环境中的适应能力</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifierclf = KNeighborsClassifier(n_neighbors=<span class="number">3</span>)clf.fit(X_train,y_train)correct = np.count_nonzero((clf.predict(X_test)==y_test==<span class="literal">True</span>)print(<span class="string">"auc is：%3.f"</span>%(corrrect/len(X_test))</span><br></pre></td></tr></table></figure><h3 id="交叉验证"><a href="#交叉验证" class="headerlink" title="交叉验证"></a>交叉验证</h3><p>把数据集分为训练集和测试集</p><blockquote><p>常用的交叉验证技术叫做K折交叉验证(K-fold Cross Validation)。 我们先把训练数据再分成训练集和验证集，之后使用训练集来训练模型，然后再验证集上评估模型的准确率。举个例子，比如一个模型有个参数叫\alphaα，我们一开始不清楚要选择0.1还是1，所以这时候我们进行了交叉验证：把所有训练集分成K块，依次对每一个\alphaα值评估它的准确率。下面的动画讲述了如何使用K折交叉验证选出最合适的参数值。  </p></blockquote><blockquote><p>leave_one_out交叉验证，也就是每次只把一个样本当做验证数据，剩下的其他数据都当做是训练样本。  </p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">form sklearn.model_selection import GirdSearchCVknn = KNeighborsClassifier()clf = GirdSearchCV(knn,parameters,cv=5)clf.fit(x,y)</span><br><span class="line">clf.best_score_clf.best_params_</span><br></pre></td></tr></table></figure><ul><li>绝对不能把测试数据用在交叉验证的过程中</li></ul><h3 id="特征缩放"><a href="#特征缩放" class="headerlink" title="特征缩放"></a>特征缩放</h3><p>目的是为了：消除有些变量变化的影响</p><p>1 线性归一化（Min-max normalization）</p><p>线性归一化指的是把特征值的范围映射到[0,1]区间</p><p>x_new  = (x - min())/(max()-min())</p><p>2 标准差标准化 （Z-score Normalization）</p><p>特征值映射到均值为0，标准差为1的正态分布 x_new = (x-mean(x)/std(x)</p><p>mean(x) x 的平均值 std(x) x的标注差</p><h2 id="KNN总结："><a href="#KNN总结：" class="headerlink" title="KNN总结："></a>KNN总结：</h2><ul><li>knn 是一个及其简单的算法</li><li>算法比较适合低纬空间</li><li>KNN 在训练过程中实质上不需要做任何事情，所以训练本身不产生任何时间上的消耗。</li></ul><p>merry-based  instance -based  (实际上没有训练学习的过程)</p><ul><li>KNN预测的时候要循环所以的样本数据，复杂度依赖于样本个数，达成KNN应用在大数据的瓶颈。。</li></ul><h3 id="结构化数据与非结构化数据"><a href="#结构化数据与非结构化数据" class="headerlink" title="结构化数据与非结构化数据"></a>结构化数据与非结构化数据</h3><p>非结构化数据：简单来讲，文本、图片、声音、视频这些都属于非结构化数据，需要做进一步的处理 结构化的数据指的是存放在数据库里的年龄，身高等这种信息</p><h3 id="图像"><a href="#图像" class="headerlink" title="图像"></a>图像</h3><p>图像来说，此过程相对简单。一般可以通过Python自带的库来读取图片，并把图片数据存放在矩阵(Matrix)或者张量(Tensor)里  <strong>- 图片是由像素来构成的，比如256*256或者128*128。两个值分别代表长宽上的像素。这个值越大图片就会越清晰。另外，对于彩色的图片，一个像素点一般由三维数组来构成，分别代表的是R,G,B三种颜色。除了RGB，其实还有其他常用的色彩空间。如果使用RGB来表示每一个像素点，一个大小为128*128像素的图片实际大小为128*128*3，是一个三维张量的形式。</strong>  </p><h4 id="图片特征"><a href="#图片特征" class="headerlink" title="图片特征"></a>图片特征</h4><ul><li><p>颜色特征(color histigram)</p></li><li><p>SIFT (Scale-invariant feature transfarm)</p><p>它是一个局部的特征，它会试图去寻找图片中的拐点这类的关键点，然后再通过一系列的处理最终得到一个SIFT向量</p></li><li><p>HOG (Histogram of Oriented Grandient) </p><p>通过计算和统计图像局部区域的梯度方向直方图来构建特征.由于HOG是在图像的局部方格单元上操作，所以它对图像几何的和光学的形变都能保持很好的不变性   </p></li></ul><h4 id="降维"><a href="#降维" class="headerlink" title="降维"></a>降维</h4><ul><li><p>对于一个中小型图片，它的大小一般大于256<em>256</em>3。如果把它转换成向量，其实维度的大小已经几十万了。这会导致消耗非常大的计算资源，所以一般情况下我们都会尝试对图片做一些降维操作。其实特征提取过程我们自然地可以理解为是降维过程。</p></li><li><p>降维操作会更好地保留图片中重要的信息，同时也帮助过滤掉无用的噪声  </p></li><li><p><strong>PCA(Principal Component Analysis)</strong>， (常用的降维工具)</p></li><li><p>它是一种无监督的学习方法，可以把高维的向量映射到低维的空间里。它的核心思路是对数据做线性的变换，然后在空间里选择信息量最大的Top K维度作为新的特征值     </p></li></ul><h3 id="matplotlib-pyplot（画图，展示图片）"><a href="#matplotlib-pyplot（画图，展示图片）" class="headerlink" title="matplotlib.pyplot（画图，展示图片）"></a>matplotlib.pyplot（画图，展示图片）</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt# 读取图片img = plt.imread(&apos;&apos;)print(img.shape)plt.imshow(im445555555555555g)</span><br></pre></td></tr></table></figure><h3 id="缺失值次5"><a href="#缺失值次5" class="headerlink" title="缺失值次5"></a>缺失值次5</h3><p>删除缺失的行 或者缺失的列</p><p>填补缺失值:</p><ul><li>均值，最小值，最大值，特殊值填充 ,中位值</li></ul><h3 id="特征编码"><a href="#特征编码" class="headerlink" title="特征编码"></a>特征编码</h3><p>把非数值型数据转为数值型</p><p><strong>数据独热编码(one-hot encoding)</strong> 标签编码</p><p>标签编码不能直接作为特征输入到模型中，</p><p>因为1,2,3,,,,连续的特征标签，模型是会认为这些类别是有大小关系的。</p><p>而独热编码是平行的。</p><blockquote><p>如果我们直接把类别特征看作是具体的数比如0，1，2… 那这时候，数与数之间是有大小关系的，比如2要大于1，1要大于0，而且这些大小相关的信息必然会用到模型当中  但这就跟原来特征的特点产生了矛盾，因为对于深度学习，数据分析来说它们之间并不存在所谓的“大小”，可以理解为平行关系。所以对于这类特征来说，直接用0，1，2.. 的方式来表示是存在问题的，所以结论是不能这么做。  </p></blockquote><p><strong>数值型的变量</strong> 可以当做特征直接输入，也可以进行离散化操作  </p>]]></content>
    
    <summary type="html">
    
      machinelearning
    
    </summary>
    
      <category term="tools" scheme="http://wvdon.com/categories/tools/"/>
    
    
      <category term="sklearn" scheme="http://wvdon.com/tags/sklearn/"/>
    
  </entry>
  
  <entry>
    <title>MarkDown语法</title>
    <link href="http://wvdon.com/2019/10/27/MarkDown%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/"/>
    <id>http://wvdon.com/2019/10/27/MarkDown快速入门/</id>
    <published>2019-10-27T15:42:57.000Z</published>
    <updated>2020-12-31T07:19:42.602Z</updated>
    
    <content type="html"><![CDATA[<h1 id="MarkDown快速入门"><a href="#MarkDown快速入门" class="headerlink" title="MarkDown快速入门"></a>MarkDown快速入门</h1><p><strong>Markdown是一种可以使用普通文本编辑器编写的标记语言，通过简单的标记语法，它可以使普通文本内容具有一定的格式。</strong></p><blockquote><p>推荐使用<a href="https://typora.io/" target="_blank" rel="noopener">Typora</a>进行编辑</p></blockquote><p>插入目录：[Toc] 自动生成目录</p><a id="more"></a><p>[TOC]</p><h2 id="标题"><a href="#标题" class="headerlink" title="标题:"></a>标题:</h2><p>H1-H6</p><p># 标题1<br>## 标题2<br>### 标题3</p><h1 id="斜体与加粗"><a href="#斜体与加粗" class="headerlink" title="斜体与加粗"></a>斜体与加粗</h1><h3 id="斜体"><a href="#斜体" class="headerlink" title="斜体"></a>斜体</h3><p>*文本* 或者 _文本_</p><p><em>文本</em></p><h3 id="加粗"><a href="#加粗" class="headerlink" title="加粗"></a>加粗</h3><p>*<em>文本*</em></p><p><strong>文本</strong></p><h2 id="插入图片与链接"><a href="#插入图片与链接" class="headerlink" title="插入图片与链接"></a>插入图片与链接</h2><h3 id="插入链接"><a href="#插入链接" class="headerlink" title="插入链接"></a>插入链接</h3><p>[链接文本标题](链接地址)</p><p><a href>链接文本标题</a></p><h3 id="插入图片"><a href="#插入图片" class="headerlink" title="插入图片"></a>插入图片</h3><p>!<a href="图片链接"></a></p><h2 id="代码语法高亮"><a href="#代码语法高亮" class="headerlink" title="代码语法高亮"></a>代码语法高亮</h2><h3 id="整体代码"><a href="#整体代码" class="headerlink" title="整体代码"></a>整体代码</h3><p>&lt;!–￼0–&gt;</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(“hello MachineLearning-Question”)</span><br></pre></td></tr></table></figure><h3 id="部分代码"><a href="#部分代码" class="headerlink" title="部分代码"></a>部分代码</h3><p>`bash run.py`</p><p><code>bash run.py</code></p><h2 id="列表"><a href="#列表" class="headerlink" title="列表"></a>列表</h2><h3 id="无序列表"><a href="#无序列表" class="headerlink" title="无序列表"></a>无序列表</h3><p>可以使用 * 或者 + -，注意之间的空格</p><p>- list1</p><p>​    - list1.1</p><p>- list2</p><p>​    - list1.2</p><ul><li>list1<ul><li>list1.1</li></ul></li><li>list2<ul><li>list1.2</li></ul></li></ul><h3 id="有序列表"><a href="#有序列表" class="headerlink" title="有序列表"></a>有序列表</h3><p>1.list1(我这里在 . 之前使用了\防止转变)</p><p>2.list2</p><ol><li><p>list1</p></li><li><p>list2</p></li></ol><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><h3 id="分割线"><a href="#分割线" class="headerlink" title="分割线"></a>分割线</h3><p>-–  或者 *****</p><hr><h3 id="高亮"><a href="#高亮" class="headerlink" title="高亮"></a>高亮</h3><p>==高亮文本==</p><p>==高亮文本==</p><h3 id="删除"><a href="#删除" class="headerlink" title="删除"></a>删除</h3><p><del>删除</del></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">~~删除~~</span><br></pre></td></tr></table></figure><h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><p>&gt; 一级引言</p><blockquote><p>一级引言</p></blockquote><p>&gt;&gt; 二级引言</p><blockquote><blockquote><p>二级引言</p></blockquote></blockquote><h3 id="更多排版规范可以参考Markdown中文文档"><a href="#更多排版规范可以参考Markdown中文文档" class="headerlink" title="更多排版规范可以参考Markdown中文文档"></a>更多排版规范可以参考<a href="https://markdown-zh.readthedocs.io/en/latest/" target="_blank" rel="noopener">Markdown中文文档</a></h3>]]></content>
    
    <summary type="html">
    
      生产力工具
    
    </summary>
    
      <category term="tools" scheme="http://wvdon.com/categories/tools/"/>
    
    
      <category term="markdown" scheme="http://wvdon.com/tags/markdown/"/>
    
  </entry>
  
  <entry>
    <title>第二章 ：k-近邻算法</title>
    <link href="http://wvdon.com/2019/10/24/machineLearning/machineLearning/2019-10-24-%E7%AC%AC%E4%BA%8C%E7%AB%A0-%EF%BC%9Ak-%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95/"/>
    <id>http://wvdon.com/2019/10/24/machineLearning/machineLearning/2019-10-24-第二章-：k-近邻算法/</id>
    <published>2019-10-24T13:04:52.000Z</published>
    <updated>2021-01-10T08:30:54.043Z</updated>
    
    <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>K最近邻(k-Nearest Neighbor，KNN)分类算法，是一个理论上比较成熟的方法，也是最简单的机器学习算法之一。该方法的思路是：如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别。</p><a id="more"></a><h2 id="KNN"><a href="#KNN" class="headerlink" title="KNN"></a>KNN</h2><p>俗话说近朱者赤近墨者黑，如果你想判断这个人是怎么样的人，你不妨先去看看他的朋友圈是怎么样的，所谓观其友，而识其人。</p><p><img src="https://img-blog.csdnimg.cn/20210110162911461.png#pic_center" alt="在这里插入图片描述"></p><p>从图中我们能够看到，如果要判断 绿色的是什么形状我们可以通过先判断他距离他最近的K个图形</p><p>当K=3,基于统计，两个三角形（2/3），一个正方形（1/3），我们可以判断绿色为三角形</p><p>当K=5，两个三角形(2/5),三个正方形(3/5),我们判断绿色为正方形</p><p>························</p><h4 id="k-近邻算法的一般流程"><a href="#k-近邻算法的一般流程" class="headerlink" title="k-近邻算法的一般流程"></a>k-近邻算法的一般流程</h4><blockquote><p>(1) 收集数据：可以使用任何方法。<br>(2) 准备数据：距离计算所需要的数值，最好是结构化的数据格式。<br>(3) 分析数据：可以使用任何方法。<br>(4) 训练算法：此步骤不适用于k-近邻算法。<br>(5) 测试算法：计算错误率。<br>(6) 使用算法：首先需要输入样本数据和结构化的输出结果，然后运行k-近邻算法判定输 入数据分别属于哪个分类，最后应用对计算出的分类执行后续的处理。</p></blockquote><h2 id="KNN简单实现-1"><a href="#KNN简单实现-1" class="headerlink" title="KNN简单实现(1)"></a>KNN简单实现(1)</h2><blockquote><p> python=3.7</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Created on 2019/10/24 by wvdon</span></span><br><span class="line"><span class="comment"># website wvdon.com</span></span><br><span class="line"><span class="comment"># 导入使用的包</span></span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> operator</span><br></pre></td></tr></table></figure><p>创建训练的数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createDataSet</span><span class="params">()</span>:</span></span><br><span class="line">    group = array([[<span class="number">1.0</span>,<span class="number">2.0</span>],[<span class="number">1.2</span>,<span class="number">0.1</span>],[<span class="number">0.1</span>,<span class="number">1.4</span>],[<span class="number">0.3</span>,<span class="number">3.5</span>]])</span><br><span class="line">    labels = [<span class="string">"A"</span>,<span class="string">"A"</span>,<span class="string">"B"</span>,<span class="string">"B"</span>]</span><br><span class="line">    <span class="keyword">return</span> group,labels</span><br></pre></td></tr></table></figure><p>利用matplotlib观察数据分布</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">group,labels = createDataSet()</span><br><span class="line">x = group[:,<span class="number">0</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y = group[:,<span class="number">1</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(x,y)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="!%5B%E5%9C%A8%E8%BF%99%E9%87%8C%E6%8F%92%E5%85%A5%E5%9B%BE%E7%89%87%E6%8F%8F%E8%BF%B0%5D(https://img-blog.csdnimg.cn/20210110163017357.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1dlRG9uX3Q=,size_16,color_FFFFFF,t_70#pic_center" alt="png"></p><h4 id="定义一个KNN函数"><a href="#定义一个KNN函数" class="headerlink" title="定义一个KNN函数"></a>定义一个KNN函数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify0</span><span class="params">(inX,dataSet,labels,k)</span>:</span></span><br><span class="line">    dataSetSize = dataSet.shape[<span class="number">0</span>] <span class="comment">#获得训练集的长度</span></span><br><span class="line">    diffMat = tile(inX,(dataSetSize,<span class="number">1</span>))-dataSet <span class="comment"># 复制数组 并将差值计算出来</span></span><br><span class="line">    <span class="comment">#下面三个是计算欧氏距离 </span></span><br><span class="line">    sqDiffMat = diffMat**<span class="number">2</span></span><br><span class="line">    sqDistances = sqDiffMat.sum(axis=<span class="number">1</span>)</span><br><span class="line">    distances = sqDistances**<span class="number">0.5</span> </span><br><span class="line">    </span><br><span class="line">    <span class="comment">#获得distances从小到大的索引值</span></span><br><span class="line">    sortedDistIndicies = distances.argsort()</span><br><span class="line">    classCount = &#123;&#125;</span><br><span class="line">    <span class="comment">#找到前K个标签，输出最大的</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line">        voteIlabel = labels[sortedDistIndicies[i]]</span><br><span class="line">        classCount [voteIlabel] = classCount.get(voteIlabel,<span class="number">0</span>)+<span class="number">1</span></span><br><span class="line">    sortedClassCount = sorted(classCount.items(),key = operator.itemgetter(<span class="number">1</span>),reverse = <span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> sortedClassCount[<span class="number">0</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = classify0([<span class="number">1.2</span>,<span class="number">0.1</span>],group,labels,<span class="number">1</span>)</span><br><span class="line">a</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&apos;A&apos;</span><br></pre></td></tr></table></figure><h4 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h4><p><strong>欧氏距离</strong>，最常见的两点之间或多点之间的距离表示法，又称之为欧几里得度量，它定义于欧几里得空间中，如点 x = (x1,…,xn),到 y(y1,….yn)</p><p><a href="https://www.codecogs.com/eqnedit.php?latex=dist(X,Y)&space;=&space;\sqrt{(x_1-y_1)^2&plus;(x_2-y_2)^2&plus;····&plus;(x_n-y_n)^2}=\sqrt{\sum_{i=1}^{n}(X_i-Y_i)^2}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?dist(X,Y)&space;=&space;\sqrt{(x_1-y_1)^2&plus;(x_2-y_2)^2&plus;····&plus;(x_n-y_n)^2}=\sqrt{\sum_{i=1}^{n}(X_i-Y_i)^2}" title="dist(X,Y) = \sqrt{(x_1-y_1)^2+(x_2-y_2)^2+····+(x_n-y_n)^2}=\sqrt{\sum_{i=1}^{n}(X_i-Y_i)^2}"></a></p><p>二位平面上 </p><p><a href="https://www.codecogs.com/eqnedit.php?latex=d&space;=&space;\sqrt{(x_1-y_1)^2&plus;(x_2-y_2)^2}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?d&space;=&space;\sqrt{(x_1-y_1)^2&plus;(x_2-y_2)^2}" title="d = \sqrt{(x_1-y_1)^2+(x_2-y_2)^2}"></a></p><h4 id="numpy-函数"><a href="#numpy-函数" class="headerlink" title="numpy 函数"></a>numpy 函数</h4><p>tile<br>sum(axis=1)<br>argsort()<br><a href>numpy总结</a></p><h2 id="使用-k-近邻算法改进约会网站的配对效果-2"><a href="#使用-k-近邻算法改进约会网站的配对效果-2" class="headerlink" title="使用 k-近邻算法改进约会网站的配对效果(2)"></a>使用 k-近邻算法改进约会网站的配对效果(2)</h2><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="[https://baike.baidu.com/item/k%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95/9512781](https://baike.baidu.com/item/k近邻算法/9512781)">百度百科</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要&quot;&gt;&lt;/a&gt;摘要&lt;/h2&gt;&lt;p&gt;K最近邻(k-Nearest Neighbor，KNN)分类算法，是一个理论上比较成熟的方法，也是最简单的机器学习算法之一。该方法的思路是：如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>第一章 ：机器学习基础</title>
    <link href="http://wvdon.com/2019/10/22/machineLearning/machineLearning/2019-10-22-%E7%AC%AC%E4%B8%80%E7%AB%A0-%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"/>
    <id>http://wvdon.com/2019/10/22/machineLearning/machineLearning/2019-10-22-第一章-：机器学习基础/</id>
    <published>2019-10-22T13:04:52.000Z</published>
    <updated>2021-01-10T08:52:27.415Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>本系列文章源自于读《机器学习实战》这本书的学习笔记，用于记录学习内容，不断更新，，</p><p>21世纪互联网的发展，使得我们对于数据的收集更加容易，在这样大量数据的前提下，我们必须要挖掘出海量数据的价值，而机器学习的算法受到许多爱好技术人员的青睐，是因为这些算法在一定程度上可以帮助我们进行一些诸如回归预测，分类，识别等诸多场景的应用。</p><a id="more"></a><h4 id="何谓数据，信息，知识呢？"><a href="#何谓数据，信息，知识呢？" class="headerlink" title="何谓数据，信息，知识呢？"></a>何谓数据，信息，知识呢？</h4><blockquote><p>一切的实物都是数据，而信息是有用的数据</p><p>例如： 通过测量行星的位置和对应的时间，我们得到的就是数据。而通过这些数据获得行星的轨迹就是信息，通过这些信息总结出来开普勒三定律就是知识了。</p></blockquote><h2 id="本书结构"><a href="#本书结构" class="headerlink" title="本书结构"></a>本书结构</h2><p>《机器学习实战》全书共15章，主要是介绍了数据挖掘的十大算法其八：</p><p>C4.5决策树、K-均值（K-mean）、支持向量机（SVM）、Apriori、 最大期望算法（EM）、PageRank算法、AdaBoost算法、k-近邻算法（kNN）、朴素贝叶斯算法（NB） 和分类回归树（CART）算法。没有包括最大期望算法和PageRank 算法。本书没有包括PageRank算法，是因为搜索引擎巨头Google引入的PageRank算法已经在很多 著作里得到了充分的论述，没有必要进一步累述；而最大期望算法没有纳入，是因为涉及太多的</p><h4 id="选择本书的原因"><a href="#选择本书的原因" class="headerlink" title="选择本书的原因"></a>选择本书的原因</h4><p>本书的实战性比较强，每个章节都是用例子来说明的，符合我这种喜欢写代码的同学，如果是西瓜书我可能看着就睡着了，理论知识太多了。</p><p>我的计划是可以通过本书对这几个算法有一定的了解之后再选择西瓜书或李宏毅的课程进行研读。</p><p>由于书上实现都是用的python2.x，我会用python3.7将其复现，并用jupyter展示出来。</p><h2 id="何谓机器学习呢？"><a href="#何谓机器学习呢？" class="headerlink" title="何谓机器学习呢？"></a>何谓机器学习呢？</h2><p>机器学习是一类算法的总称，这些算法企图从大量历史数据中挖掘出其中隐含的规律，并用于预测或者分类，更具体的说，机器学习可以看作是寻找一个函数，输入是样本数据，输出是期望的结果，只是这个函数过于复杂，以至于不太方便形式化表达。需要注意的是，机器学习的目标是使学到的函数很好地适用于“新样本”，而不仅仅是在训练样本上表现很好。学到的函数适用于新样本的能力，称为泛化（Generalization）能力。</p><p>简言之，机器学习可以揭示数据背后的真实 含义</p><h2 id="人工智能-机器学习-深度学习"><a href="#人工智能-机器学习-深度学习" class="headerlink" title="人工智能 机器学习 深度学习"></a>人工智能 机器学习 深度学习</h2><p><b>人工智能是追求目标，机器学习是实现手段，深度学习是其中一种方法。</b></p><h3 id="人工智能（Artificial-Intelligence）：机器赋予人的智慧"><a href="#人工智能（Artificial-Intelligence）：机器赋予人的智慧" class="headerlink" title="人工智能（Artificial Intelligence）：机器赋予人的智慧"></a>人工智能<strong>（Artificial Intelligence）</strong>：机器赋予人的智慧</h3><p>1956年，几个计算机科学家相聚在达特茅斯会议，提出了“人工智能”的概念，梦想着用当时刚刚出现的计算机来构造复杂的、拥有与人类智慧同样本质特性的机器<br>2012年以后，得益于数据量的上涨、运算力的提升和机器学习新算法（深度学习）的出现，人工智能开始大爆发。</p><p>现在人工智能的研究领域只要有 专家系统、机器学习、进化计算、模糊逻辑、计算机视觉、自然语言处理、推荐系统等</p><h5 id="弱人工智能-强人工智能"><a href="#弱人工智能-强人工智能" class="headerlink" title="弱人工智能-强人工智能"></a>弱人工智能-强人工智能</h5><p>我们现在所说的人工智能都是弱人工智能，能够实现具体的某一种任务，如人脸识别，机器翻译。<br>墙人工智能大概就像是人一样吧，能够自己学习everything</p><h3 id="机器学习（Machine-Learning）：一种实现人工智能的手段"><a href="#机器学习（Machine-Learning）：一种实现人工智能的手段" class="headerlink" title="机器学习（Machine Learning）：一种实现人工智能的手段"></a>机器学习（Machine Learning）：一种实现人工智能的手段</h3><p>机器学习的最基本做法，便是使用算法来对数据进行解析、学习，然后对真实世界中的数据/事件作出决策/预测。</p><p>根据<strong>使用算法</strong>的不同，机器学习的算法可包括：决策树、聚类、支持向量机、朴素贝叶斯等。</p><p>根据<strong>学习方法</strong>的不同，机器学习可以分为：监督学习、半监督学习、无监督学习、集成学习等。</p><h3 id="深度学习-Deep-Learning-：一种实现机器学习的方法"><a href="#深度学习-Deep-Learning-：一种实现机器学习的方法" class="headerlink" title="深度学习(Deep Learning)：一种实现机器学习的方法"></a>深度学习(Deep Learning)：一种实现机器学习的方法</h3><p>对机器学习来说，特征提取并不简单。特征工程往往需要大量的时间去优化，而此时，深度学习便可以自动学习特征和任务之间的关联，还能从简单特征中提取复杂的特征。深</p><p>度学习的概念源于人工神经网络的研究，含多隐层的多层感知器就是一种深度学习结构，深度学习通过组合低层特征形成更加抽象的高层表示属性类别或特征，以发现数据的分布式特征表示</p><h3 id="三者联系"><a href="#三者联系" class="headerlink" title="三者联系"></a>三者联系</h3><p><img src="https://img-blog.csdnimg.cn/20210110163343351.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1dlRG9uX3Q=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p>人工智能 &gt;&gt; 机器学习 &gt;&gt; 深度学习</p><h2 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h2><p>机器学习的任务个人理解是主要做分类与回归。分类和回归属于监督学习，之所以称 之为监督学习，是因为这类算法必须知道预测什么，即目标变量的分类信息。</p><p>与监督学习相对应的是无监督学习，此时数据没有类别信息，也不会给定目标值。对于离散型的遍历</p><h4 id="选择合适的算法："><a href="#选择合适的算法：" class="headerlink" title="选择合适的算法："></a>选择合适的算法：</h4><p>如果想要预测目标变量的值，则可以选择监督学习算法， 否则可以选择无监督学习算法。确定选择监督学习算法之后，需要进一步确定目标变量类型，如 果目标变量是离散型，如是/否、1/2/3、A/B/C或者红/黄/黑等，则可以选择分类器算法；如果目 标变量是连续型的数值，如0.0～100.00、999～999或者+∞～-∞等，则需要选择回归算法</p><blockquote><p> 离散型：分类器</p><p>连续性：逻辑回归</p></blockquote><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://blog.csdn.net/hohaizx/article/details/80584307" target="_blank" rel="noopener">机器学习简介</a></p><p><a href="https://www.zhihu.com/question/57770020" target="_blank" rel="noopener">知乎-人工智能-机器学习-深度学习的区别</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;本系列文章源自于读《机器学习实战》这本书的学习笔记，用于记录学习内容，不断更新，，&lt;/p&gt;&lt;p&gt;21世纪互联网的发展，使得我们对于数据的收集更加容易，在这样大量数据的前提下，我们必须要挖掘出海量数据的价值，而机器学习的算法受到许多爱好技术人员的青睐，是因为这些算法在一定程度上可以帮助我们进行一些诸如回归预测，分类，识别等诸多场景的应用。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Linux</title>
    <link href="http://wvdon.com/2019/10/18/linux/2019-10-18-Linux/"/>
    <id>http://wvdon.com/2019/10/18/linux/2019-10-18-Linux/</id>
    <published>2019-10-18T07:50:39.000Z</published>
    <updated>2021-01-10T08:53:18.060Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://img-blog.csdnimg.cn/20210110163536333.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1dlRG9uX3Q=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><blockquote><p>unnatural 石原里美 入坑了她的美</p></blockquote><h2 id="Linux基本知识"><a href="#Linux基本知识" class="headerlink" title="Linux基本知识"></a>Linux基本知识</h2><h5 id="查询命令-man"><a href="#查询命令-man" class="headerlink" title="查询命令 man"></a>查询命令 man</h5><p>linux的命令和参数太多，而且容易记错，我们可以通过man [命令] 来查看如何命令的使用文档</p><h2 id="Linux的文件权限和目录配置"><a href="#Linux的文件权限和目录配置" class="headerlink" title="Linux的文件权限和目录配置"></a>Linux的文件权限和目录配置</h2><a id="more"></a><h5 id="用户组与用户"><a href="#用户组与用户" class="headerlink" title="用户组与用户"></a>用户组与用户</h5><p><img src="!%5B%E5%9C%A8%E8%BF%99%E9%87%8C%E6%8F%92%E5%85%A5%E5%9B%BE%E7%89%87%E6%8F%8F%E8%BF%B0%5D(https://img-blog.csdnimg.cn/20210110163703465.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1dlRG9uX3Q=,size_16,color_FFFFFF,t_70#pic_center" alt="1571385663320"></p><p>其中 老王家就代表一个用户组 ，单个人代表为用户。</p><blockquote><ul><li>linux的用户是记录在<br>/etc/passwd</li><li>密码记录在<br>/etc/shadow</li><li>所有的组记录在<br>/etc/group</li></ul></blockquote><h6 id="添加新用户-："><a href="#添加新用户-：" class="headerlink" title="添加新用户 ："></a>添加新用户 ：</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">adduser wvdon </span><br><span class="line"><span class="meta">#</span>修改所添加用户的密码</span><br><span class="line">passwd wvdon</span><br></pre></td></tr></table></figure><h5 id="文件权限"><a href="#文件权限" class="headerlink" title="文件权限"></a>文件权限</h5><p>文件的权限被分为 可读(read) 可写(write) 可执行（execute） 简称为 r w x</p><p>用数字表示 r:4 w:2 x:1</p><p>文件显示的权限</p><blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span>  -rw-rw-r-- 1 wuweidong061 wvdon    0 Oct 18 23:08 testfile.txt</span><br><span class="line"><span class="meta">&gt;</span></span><br></pre></td></tr></table></figure></blockquote><blockquote><blockquote><p>①第一个符合 为 d 或者 - 分别代表 文件夹或者文件<br>②2-4符号代表所属用户的权限 rw- 即可读可写（4+2+0）<br>③5-7符号代表所属用户组的权限 rw- 即可读可写（4+2+0）<br>④8-10符号代表其他用户的权限为r–仅读(4)<br>⑤代表连接数<br>⑥代表 文件所有者 wuweidong061<br>⑦代表 文件所属用户组 wvdon<br>⑧时间代表文件最后修改的时间<br>⑨文件名</p></blockquote></blockquote><h5 id="修改文件用户与权限的三个命令"><a href="#修改文件用户与权限的三个命令" class="headerlink" title="修改文件用户与权限的三个命令"></a>修改文件用户与权限的三个命令</h5><h6 id="chgrp"><a href="#chgrp" class="headerlink" title="chgrp"></a>chgrp</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span>chgrp=change group</span><br><span class="line"><span class="meta">#</span>修改用户组</span><br><span class="line">-R 递归更改(连着文件夹下的目录和文件都进行更改)</span><br><span class="line">chgrp user filename</span><br></pre></td></tr></table></figure><h6 id="chown"><a href="#chown" class="headerlink" title="chown"></a>chown</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span>chown=change owner</span><br><span class="line"><span class="meta">#</span>修改文件所属者</span><br><span class="line">chown [-R] 账户名称 文件或目录</span><br></pre></td></tr></table></figure><h6 id="chmod"><a href="#chmod" class="headerlink" title="chmod"></a>chmod</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span>chmod = chmod mode</span><br><span class="line"><span class="meta">#</span>更改文件的权限</span><br><span class="line"><span class="meta">#</span>一个文件共有三种权限分别是w r x，同时一个文件也对三种用户组状态设置权限 所有者用户② 用户组③ 其他用户④</span><br><span class="line"><span class="meta">#</span>更改权限的方法有三种，</span><br><span class="line"><span class="meta">#</span>1 利用对应的数值</span><br><span class="line">chmod 777 filename</span><br><span class="line"><span class="meta">#</span>2 利用对应的身份状态设置</span><br><span class="line"><span class="meta">#</span> a u g o代表 all[所有用户] user[文件所有者] group[用户所有组] other[其他用户] </span><br><span class="line"><span class="meta">#</span>设置类型可以为 + - =</span><br><span class="line">chmod u=rwx,g=rx,o=x filename</span><br><span class="line"><span class="meta">#</span>增加或者减少文件的权限 可以通过 </span><br><span class="line"><span class="meta">#</span>chmod [用户类型][设置类型 + -][权限w r x] filename</span><br><span class="line"><span class="meta">#</span>例如添加w或者减去x</span><br><span class="line">chmod a+w filename</span><br><span class="line">chmod a-x filename</span><br></pre></td></tr></table></figure><h5 id="对文件与目录的常见操作"><a href="#对文件与目录的常见操作" class="headerlink" title="对文件与目录的常见操作"></a>对文件与目录的常见操作</h5><h3 id="1-ls"><a href="#1-ls" class="headerlink" title="1. ls"></a>1. ls</h3><p>列出文件或者目录的信息，目录的信息就是其中包含的文件。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># ls [-aAdfFhilnrRSt] file|dir</span><br><span class="line">-a ：列出全部的文件</span><br><span class="line">-d ：仅列出目录本身</span><br><span class="line">-l ：以长数据串行列出，包含文件的属性与权限等等数据</span><br></pre></td></tr></table></figure><h3 id="2-cd"><a href="#2-cd" class="headerlink" title="2. cd"></a>2. cd</h3><p>更换当前目录。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd [相对路径或绝对路径]</span><br></pre></td></tr></table></figure><h3 id="3-mkdir"><a href="#3-mkdir" class="headerlink" title="3. mkdir"></a>3. mkdir</h3><p>创建目录。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># mkdir [-mp] 目录名称</span><br><span class="line">-m ：配置目录权限</span><br><span class="line">-p ：递归创建目录</span><br></pre></td></tr></table></figure><h3 id="4-rmdir"><a href="#4-rmdir" class="headerlink" title="4. rmdir"></a>4. rmdir</h3><p>删除目录，目录必须为空。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rmdir [-p] 目录名称</span><br><span class="line">-p ：递归删除目录</span><br></pre></td></tr></table></figure><h3 id="5-touch"><a href="#5-touch" class="headerlink" title="5. touch"></a>5. touch</h3><p>更新文件时间或者建立新文件。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># touch [-acdmt] filename</span><br><span class="line">-a ： 更新 atime</span><br><span class="line">-c ： 更新 ctime，若该文件不存在则不建立新文件</span><br><span class="line">-m ： 更新 mtime</span><br><span class="line">-d ： 后面可以接更新日期而不使用当前日期，也可以使用 --date=&quot;日期或时间&quot;</span><br><span class="line">-t ： 后面可以接更新时间而不使用当前时间，格式为[YYYYMMDDhhmm]</span><br></pre></td></tr></table></figure><h3 id="6-cp"><a href="#6-cp" class="headerlink" title="6. cp"></a>6. cp</h3><p>复制文件。如果源文件有两个以上，则目的文件一定要是目录才行。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cp [-adfilprsu] source destination</span><br><span class="line">-a ：相当于 -dr --preserve=all</span><br><span class="line">-d ：若来源文件为链接文件，则复制链接文件属性而非文件本身</span><br><span class="line">-i ：若目标文件已经存在时，在覆盖前会先询问</span><br><span class="line">-p ：连同文件的属性一起复制过去</span><br><span class="line">-r ：递归复制</span><br><span class="line">-u ：destination 比 source 旧才更新 destination，或 destination 不存在的情况下才复制</span><br><span class="line">--preserve=all ：除了 -p 的权限相关参数外，还加入 SELinux 的属性, links, xattr 等也复制了</span><br></pre></td></tr></table></figure><h3 id="7-rm"><a href="#7-rm" class="headerlink" title="7. rm"></a>7. rm</h3><p>删除文件。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># rm [-fir] 文件或目录</span><br><span class="line">-r ：递归删除</span><br></pre></td></tr></table></figure><h3 id="8-mv"><a href="#8-mv" class="headerlink" title="8. mv"></a>8. mv</h3><p>移动文件。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># mv [-fiu] source destination</span><br><span class="line"># mv [options] source1 source2 source3 .... directory</span><br><span class="line">-f ： force 强制的意思，如果目标文件已经存在，不会询问而直接覆盖</span><br></pre></td></tr></table></figure><h2 id="附表"><a href="#附表" class="headerlink" title="附表"></a>附表</h2><h3 id="命令缩写一览表"><a href="#命令缩写一览表" class="headerlink" title="命令缩写一览表"></a>命令缩写一览表</h3><p><a href="../linux/2019-10-17-Linux命令缩写一览表/">链接</a></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>鸟哥的linux私房菜 第三版 </p><p><a href="https://cyc2018.github.io/CS-Notes/#/notes/Linux" target="_blank" rel="noopener">https://cyc2018.github.io/CS-Notes/#/notes/Linux</a></p><p><a href="https://blog.csdn.net/sinat_29742125/article/details/52818115" target="_blank" rel="noopener">命令缩写</a></p>]]></content>
    
    <summary type="html">
    
      生产力工具
    
    </summary>
    
      <category term="tools" scheme="http://wvdon.com/categories/tools/"/>
    
    
      <category term="linux" scheme="http://wvdon.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>Linux命令缩写一揽表</title>
    <link href="http://wvdon.com/2019/10/18/linux/2019-10-17-Linux%E5%91%BD%E4%BB%A4%E7%BC%A9%E5%86%99%E4%B8%80%E8%A7%88%E8%A1%A8/"/>
    <id>http://wvdon.com/2019/10/18/linux/2019-10-17-Linux命令缩写一览表/</id>
    <published>2019-10-18T07:50:39.000Z</published>
    <updated>2020-12-31T07:19:42.605Z</updated>
    
    <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>每每都会忘记linux命令，不过我记得全拼的时候可能就能想起来了。                                                                                                     </p><a id="more"></a><p>​                                                                                                                                                                              </p><blockquote><p>ls：list(列出目录内容)</p><p>cd：Change Directory（改变目录）</p><p>su:switch user 切换用户</p><p>rpm:redhat package manager 红帽子打包管理器</p><p>pwd:print work directory 打印当前目录显示出当前工作目录的绝对路径</p><p>ps: process status(进程状态，类似于 windows 的任务管理器)</p><p>常用参数：－auxf</p><p>ps -auxf 显示进程状态</p><p>df: disk free 其功能是显示磁盘可用空间数目信息及空间结点信息。换句话说，就是报告在任何安装的设备或目录中，还剩多少自由的空间。</p><p>rpm：即 RedHat Package Management，是 RedHat 的发明之一</p><p>rmdir：Remove Directory（删除目录）</p><p>rm：Remove（删除目录或文件）</p><p>cat: concatenate 连锁 cat file1 file2&gt;&gt;file3把文件1和文件2的内容联合起来放到 file3中</p><p>insmod: install module,载入模块</p><p>ln -s : link -soft 创建一个软链接，相当于创建一个快捷方式</p><p>mkdir：Make Directory(创建目录</p><p>touch</p><p>man: Manual</p><p>pwd：Print working directory</p><p>su：Swith user</p><p>cd：Change directory</p><p>ls：List files</p><p>ps：Process Status</p><p>mkdir：Make directory</p><p>rmdir：Remove directory</p><p>mkfs: Make file system</p><p>fsck：File system check</p><p>cat: Concatenate</p><p>uname: Unix name</p><p>df: Disk free</p><p>du: Disk usage</p><p>lsmod: List modules</p><p>mv: Move file</p><p>rm: Remove file</p><p>cp: Copy file</p><p>ln: Link files</p><p>fg: Foreground</p><p>bg: Background</p><p>chown: Change owner</p><p>chgrp: Change group</p><p>chmod: Change mode</p><p>umount: Unmount</p><p>dd: 本来应根据其功能描述“Convert an copy”命名为“cc”，但“cc”已经被用以代表“C Complier”，所以命名为“dd”</p><p>tar：Tape archive</p><p>ldd：List dynamic dependencies</p><p>insmod：Install module</p><p>rmmod：Remove module</p><p>lsmod：List module</p><p>文件结尾的”rc”（如.bashrc、.xinitrc 等）：Resource configuration</p><p>Knnxxx / Snnxxx（位于 rcx.d 目录下）：K（Kill）；S(Service)；nn（执行顺序号）；xxx（服务标识）</p><p>.a（扩展名 a）：Archive，static library</p><p>.so（扩展名 so）：Shared object，dynamically linked library</p><p>.o（扩展名 o）：Object file，complied result of C/C++ source file</p><p>RPM：Red hat package manager</p><p>dpkg：Debian package manager</p><p>apt：Advanced package tool（Debian 或基于 Debian 的发行版中提供）</p><p>bin = BINaries</p><p>/dev = DEVices</p><p>/etc = ETCetera</p><p>/lib = LIBrary</p><p>/proc = PROCesses</p><p>/sbin = Superuser BINaries</p><p>/tmp = TeMPorary</p><p>/usr = Unix Shared Resources</p><p>/var = VARiable ?</p><p>FIFO = First In, First Out</p><p>GRUB = GRand Unified Bootloader</p><p>IFS = Internal Field Seperators</p><p>LILO = LInux LOader</p><p>MySQL = My 是最初作者女儿的名字，SQL = Structured Query Language</p><p>PHP = Personal Home Page Tools = PHP Hypertext Preprocessor</p><p>PS = Prompt String</p><p>Perl = “Pratical Extraction and Report Language” = “Pathologically Eclectic Rubbish Lister”</p><p>Python 得名于电视剧 Monty Python’s Flying Circus</p><p>Tcl = Tool Command Language</p><p>Tk = ToolKit</p><p>VT = Video Terminal</p><p>YaST = Yet Another Setup Tool</p><p>apache = “a patchy” server</p><p>apt = Advanced Packaging Tool</p><p>ar = archiver</p><p>as = assembler</p><p>awk = “Aho Weiberger and Kernighan” 三个作者的姓的第一个字母</p><p>bash = Bourne Again SHell</p><p>bc = Basic (Better) Calculator</p><p>bg = BackGround</p><p>biff = 作者 Heidi Stettner 在 U.C.Berkely 养的一条狗,喜欢对邮递员汪汪叫。</p><p>cal = CALendar</p><p>cat = CATenate</p><p>cd = Change Directory</p><p>chgrp = CHange GRouP</p><p>chmod = CHange MODe</p><p>chown = CHange OWNer</p><p>chsh = CHange SHell</p><p>cmp = compare</p><p>cobra = Common Object Request Broker Architecture</p><p>comm = common</p><p>cp = CoPy</p><p>cpio = CoPy In and Out</p><p>cpp = C Pre Processor</p><p>cron = Chronos 希腊文时间</p><p>cups = Common Unix Printing System</p><p>cvs = Current Version System</p><p>daemon = Disk And Execution MONitor</p><p>dc = Desk Calculator</p><p>dd = Disk Dump</p><p>df = Disk Free</p><p>diff = DIFFerence</p><p>dmesg = diagnostic message</p><p>du = Disk Usage</p><p>ed = editor</p><p>egrep = Extended GREP</p><p>elf = Extensible Linking Format</p><p>elm = ELectronic Mail</p><p>emacs = Editor MACroS</p><p>eval = EVALuate</p><p>ex = EXtended</p><p>exec = EXECute</p><p>fd = file descriptors</p><p>fg = ForeGround</p><p>fgrep = Fixed GREP</p><p>fmt = format</p><p>fsck = File System ChecK</p><p>fstab = FileSystem TABle</p><p>fvwm = F*** Virtual Window Manager</p><p>gawk = GNU AWK</p><p>gpg = GNU Privacy Guard</p><p>groff = GNU troff</p><p>hal = Hardware Abstraction Layer</p><p>joe = Joe’s Own Editor</p><p>ksh = Korn SHell</p><p>lame = Lame Ain’t an MP3 Encoder</p><p>lex = LEXical analyser</p><p>lisp = LISt Processing = Lots of Irritating Superfluous Parentheses</p><p>ln = LiNk</p><p>lpr = Line PRint</p><p>ls = list</p><p>lsof = LiSt Open Files</p><p>m4 = Macro processor Version 4</p><p>man = MANual pages</p><p>mawk = Mike Brennan’s AWK</p><p>mc = Midnight Commander</p><p>mkfs = MaKe FileSystem</p><p>mknod = MaKe NODe</p><p>motd = Message of The Day</p><p>mozilla = MOsaic GodZILLa</p><p>mtab = Mount TABle</p><p>mv = MoVe</p><p>nano = Nano’s ANOther editor</p><p>nawk = New AWK</p><p>nl = Number of Lines</p><p>nm = names</p><p>nohup = No HangUP</p><p>nroff = New ROFF</p><p>od = Octal Dump</p><p>passwd = PASSWorD</p><p>pg = pager</p><p>pico = PIne’s message COmposition editor</p><p>pine = “Program for Internet News &amp; Email” = “Pine is not Elm”</p><p>ping = 拟声又 = Packet InterNet Grouper</p><p>pirntcap = PRINTer CAPability</p><p>popd = POP Directory</p><p>pr = pre</p><p>printf = PRINT Formatted</p><p>ps = Processes Status</p><p>pty = pseudo tty</p><p>pushd = PUSH Directory</p><p>pwd = Print Working Directory</p><p>rc = runcom = run command, rc 还是 plan9的 shell</p><p>rev = REVerse</p><p>rm = ReMove</p><p>rn = Read News</p><p>roff = RunOFF</p><p>rpm = RPM Package Manager = RedHat Package Manager</p><p>rsh, rlogin, rvim 中的 r = Remote</p><p>rxvt = ouR XVT</p><p>seamoneky = 我</p><p>sed = Stream EDitor</p><p>seq = SEQuence</p><p>shar = SHell ARchive</p><p>slrn = S-Lang rn</p><p>ssh = Secure SHell</p><p>ssl = Secure Sockets Layer</p><p>stty = Set TTY</p><p>su = Substitute User</p><p>svn = SubVersioN</p><p>tar = Tape ARchive</p><p>tcsh = TENEX C shell</p><p>tee = T (T 形水管接口)</p><p>telnet = TEminaL over Network</p><p>termcap = terminal capability</p><p>terminfo = terminal information</p><p>tex = τ</p></blockquote>]]></content>
    
    <summary type="html">
    
      生产力工具
    
    </summary>
    
      <category term="tools" scheme="http://wvdon.com/categories/tools/"/>
    
    
      <category term="linux" scheme="http://wvdon.com/tags/linux/"/>
    
  </entry>
  
</feed>
