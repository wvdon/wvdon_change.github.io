---
title:机器学习基础
date: 2019-10-22 21:04:52
---

# 1. 机器学习基础

## 前言

本系列文章源自于读《机器学习实战》这本书的学习笔记，用于记录学习内容，不断更新，，

21世纪互联网的发展，使得我们对于数据的收集更加容易，在这样大量数据的前提下，我们必须要挖掘出海量数据的价值，而机器学习的算法受到许多爱好技术人员的青睐，是因为这些算法在一定程度上可以帮助我们进行一些诸如回归预测，分类，识别等诸多场景的应用。

<!-- more -->

#### 何谓数据，信息，知识呢？

> 一切的实物都是数据，而信息是有用的数据
>
> 例如： 通过测量行星的位置和对应的时间，我们得到的就是数据。而通过这些数据获得行星的轨迹就是信息，通过这些信息总结出来开普勒三定律就是知识了。

## 本书结构

《机器学习实战》全书共15章，主要是介绍了数据挖掘的十大算法其八：

C4.5决策树、K-均值（K-mean）、支持向量机（SVM）、Apriori、 最大期望算法（EM）、PageRank算法、AdaBoost算法、k-近邻算法（kNN）、朴素贝叶斯算法（NB） 和分类回归树（CART）算法。没有包括最大期望算法和PageRank 算法。本书没有包括PageRank算法，是因为搜索引擎巨头Google引入的PageRank算法已经在很多 著作里得到了充分的论述，没有必要进一步累述；而最大期望算法没有纳入，是因为涉及太多的

####  选择本书的原因

本书的实战性比较强，每个章节都是用例子来说明的，符合我这种喜欢写代码的同学，如果是西瓜书我可能看着就睡着了，理论知识太多了。

我的计划是可以通过本书对这几个算法有一定的了解之后再选择西瓜书或李宏毅的课程进行研读。

由于书上实现都是用的python2.x，我会用python3.7将其复现，并用jupyter展示出来。

## 何谓机器学习呢？

机器学习是一类算法的总称，这些算法企图从大量历史数据中挖掘出其中隐含的规律，并用于预测或者分类，更具体的说，机器学习可以看作是寻找一个函数，输入是样本数据，输出是期望的结果，只是这个函数过于复杂，以至于不太方便形式化表达。需要注意的是，机器学习的目标是使学到的函数很好地适用于“新样本”，而不仅仅是在训练样本上表现很好。学到的函数适用于新样本的能力，称为泛化（Generalization）能力。

简言之，机器学习可以揭示数据背后的真实 含义

## 人工智能 机器学习 深度学习 

<b>人工智能是追求目标，机器学习是实现手段，深度学习是其中一种方法。</b>

### 人工智能**（Artificial Intelligence）**：机器赋予人的智慧

1956年，几个计算机科学家相聚在达特茅斯会议，提出了“人工智能”的概念，梦想着用当时刚刚出现的计算机来构造复杂的、拥有与人类智慧同样本质特性的机器
2012年以后，得益于数据量的上涨、运算力的提升和机器学习新算法（深度学习）的出现，人工智能开始大爆发。

现在人工智能的研究领域只要有 专家系统、机器学习、进化计算、模糊逻辑、计算机视觉、自然语言处理、推荐系统等

##### 弱人工智能-强人工智能

我们现在所说的人工智能都是弱人工智能，能够实现具体的某一种任务，如人脸识别，机器翻译。
墙人工智能大概就像是人一样吧，能够自己学习everything

### 机器学习（Machine Learning）：一种实现人工智能的手段

机器学习的最基本做法，便是使用算法来对数据进行解析、学习，然后对真实世界中的数据/事件作出决策/预测。

根据**使用算法**的不同，机器学习的算法可包括：决策树、聚类、支持向量机、朴素贝叶斯等。

根据**学习方法**的不同，机器学习可以分为：监督学习、半监督学习、无监督学习、集成学习等。

### 深度学习(Deep Learning)：一种实现机器学习的方法

对机器学习来说，特征提取并不简单。特征工程往往需要大量的时间去优化，而此时，深度学习便可以自动学习特征和任务之间的关联，还能从简单特征中提取复杂的特征。深

度学习的概念源于人工神经网络的研究，含多隐层的多层感知器就是一种深度学习结构，深度学习通过组合低层特征形成更加抽象的高层表示属性类别或特征，以发现数据的分布式特征表示

### 三者联系

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210110163343351.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1dlRG9uX3Q=,size_16,color_FFFFFF,t_70#pic_center)

人工智能 >> 机器学习 >> 深度学习

## 补充

机器学习的任务个人理解是主要做分类与回归。分类和回归属于监督学习，之所以称 之为监督学习，是因为这类算法必须知道预测什么，即目标变量的分类信息。

与监督学习相对应的是无监督学习，此时数据没有类别信息，也不会给定目标值。对于离散型的遍历

#### 选择合适的算法：
如果想要预测目标变量的值，则可以选择监督学习算法， 否则可以选择无监督学习算法。确定选择监督学习算法之后，需要进一步确定目标变量类型，如 果目标变量是离散型，如是/否、1/2/3、A/B/C或者红/黄/黑等，则可以选择分类器算法；如果目 标变量是连续型的数值，如0.0～100.00、999～999或者+∞～-∞等，则需要选择回归算法

>  离散型：分类器
>
> 连续性：逻辑回归

# 2. k-近邻算法

## 摘要

K最近邻(k-Nearest Neighbor，KNN)分类算法，是一个理论上比较成熟的方法，也是最简单的机器学习算法之一。该方法的思路是：如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别。

<!-- more -->

## KNN

俗话说近朱者赤近墨者黑，如果你想判断这个人是怎么样的人，你不妨先去看看他的朋友圈是怎么样的，所谓观其友，而识其人。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210110162911461.png#pic_center)

从图中我们能够看到，如果要判断 绿色的是什么形状我们可以通过先判断他距离他最近的K个图形

当K=3,基于统计，两个三角形（2/3），一个正方形（1/3），我们可以判断绿色为三角形

当K=5，两个三角形(2/5),三个正方形(3/5),我们判断绿色为正方形

························

#### k-近邻算法的一般流程

> (1) 收集数据：可以使用任何方法。 
> (2) 准备数据：距离计算所需要的数值，最好是结构化的数据格式。 
> (3) 分析数据：可以使用任何方法。 
> (4) 训练算法：此步骤不适用于k-近邻算法。 
> (5) 测试算法：计算错误率。 
> (6) 使用算法：首先需要输入样本数据和结构化的输出结果，然后运行k-近邻算法判定输 入数据分别属于哪个分类，最后应用对计算出的分类执行后续的处理。

## KNN简单实现(1)

>  python=3.7

```python
# Created on 2019/10/24 by wvdon
# website wvdon.com
# 导入使用的包
from numpy import *
import operator
```

创建训练的数据

```python
def createDataSet():
    group = array([[1.0,2.0],[1.2,0.1],[0.1,1.4],[0.3,3.5]])
    labels = ["A","A","B","B"]
    return group,labels
```

利用matplotlib观察数据分布

```python
from matplotlib import pyplot as plt
group,labels = createDataSet()
x = group[:,0]
```

```python
y = group[:,1]
```

```python
plt.scatter(x,y)
plt.show()
```

![png](![在这里插入图片描述](https://img-blog.csdnimg.cn/20210110163017357.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1dlRG9uX3Q=,size_16,color_FFFFFF,t_70#pic_center)

#### 定义一个KNN函数

```python
def classify0(inX,dataSet,labels,k):
    dataSetSize = dataSet.shape[0] #获得训练集的长度
    diffMat = tile(inX,(dataSetSize,1))-dataSet # 复制数组 并将差值计算出来
    #下面三个是计算欧氏距离 
    sqDiffMat = diffMat**2
    sqDistances = sqDiffMat.sum(axis=1)
    distances = sqDistances**0.5 
    
    #获得distances从小到大的索引值
    sortedDistIndicies = distances.argsort()
    classCount = {}
    #找到前K个标签，输出最大的
    for i in range(k):
        voteIlabel = labels[sortedDistIndicies[i]]
        classCount [voteIlabel] = classCount.get(voteIlabel,0)+1
    sortedClassCount = sorted(classCount.items(),key = operator.itemgetter(1),reverse = True)
    return sortedClassCount[0][0]
```

```python
a = classify0([1.2,0.1],group,labels,1)
a
```

```
'A'
```

#### 补充

**欧氏距离**，最常见的两点之间或多点之间的距离表示法，又称之为欧几里得度量，它定义于欧几里得空间中，如点 x = (x1,...,xn),到 y(y1,....yn)

<a href="https://www.codecogs.com/eqnedit.php?latex=dist(X,Y)&space;=&space;\sqrt{(x_1-y_1)^2&plus;(x_2-y_2)^2&plus;····&plus;(x_n-y_n)^2}=\sqrt{\sum_{i=1}^{n}(X_i-Y_i)^2}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?dist(X,Y)&space;=&space;\sqrt{(x_1-y_1)^2&plus;(x_2-y_2)^2&plus;····&plus;(x_n-y_n)^2}=\sqrt{\sum_{i=1}^{n}(X_i-Y_i)^2}" title="dist(X,Y) = \sqrt{(x_1-y_1)^2+(x_2-y_2)^2+····+(x_n-y_n)^2}=\sqrt{\sum_{i=1}^{n}(X_i-Y_i)^2}" /></a>

二位平面上 

<a href="https://www.codecogs.com/eqnedit.php?latex=d&space;=&space;\sqrt{(x_1-y_1)^2&plus;(x_2-y_2)^2}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?d&space;=&space;\sqrt{(x_1-y_1)^2&plus;(x_2-y_2)^2}" title="d = \sqrt{(x_1-y_1)^2+(x_2-y_2)^2}" /></a>

# 3. 决策树

## 摘要

决策树算法类似于数据结构中的二分查找，可以使不同类型的数据集合，建立不同的分类器，最终可以通过决策树给出近似正确的结果。

<!-- more -->

## 决策树 Decision Tree 

例如一个邮件系统，通过数个问题推断，不断进行缩小答案。

 ![决策树-流程图](https://camo.githubusercontent.com/baee424187b6840aff667d476fa47d2f746f143fbd5ba8f5f78e39d71b1e81fe/687474703a2f2f646174612e617061636865636e2e6f72672f696d672f41694c6561726e696e672f6d6c2f332e4465636973696f6e547265652f2545352538362542332545372541442539362545362541302539312d2545362542352538312545372541382538422545352539422542452e6a7067) 

## 构建 The process of building a Decision Tree 

包括 特征选择、决策树的生成和决策树的修剪。 

算法：C4.5和CART

## 总结：

相比KNN算法，可以使用不同的数据集合。

对中间值的缺失不敏感，可以处理不相关的特征值、

可能会产生过度匹配问题。容易过拟合。



## 参考

[机器学习简介](https://blog.csdn.net/hohaizx/article/details/80584307)

[知乎-人工智能-机器学习-深度学习的区别](https://www.zhihu.com/question/57770020)

[百度百科]([https://baike.baidu.com/item/k%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95/9512781](https://baike.baidu.com/item/k近邻算法/9512781))