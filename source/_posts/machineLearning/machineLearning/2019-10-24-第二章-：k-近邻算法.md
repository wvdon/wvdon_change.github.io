---
title: 第二章 ：k-近邻算法
date: 2019-10-24 21:04:52
---

## 摘要

K最近邻(k-Nearest Neighbor，KNN)分类算法，是一个理论上比较成熟的方法，也是最简单的机器学习算法之一。该方法的思路是：如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别。

<!-- more -->

## KNN

俗话说近朱者赤近墨者黑，如果你想判断这个人是怎么样的人，你不妨先去看看他的朋友圈是怎么样的，所谓观其友，而识其人。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210110162911461.png#pic_center)

从图中我们能够看到，如果要判断 绿色的是什么形状我们可以通过先判断他距离他最近的K个图形

当K=3,基于统计，两个三角形（2/3），一个正方形（1/3），我们可以判断绿色为三角形

当K=5，两个三角形(2/5),三个正方形(3/5),我们判断绿色为正方形

························

#### k-近邻算法的一般流程

> (1) 收集数据：可以使用任何方法。 
> (2) 准备数据：距离计算所需要的数值，最好是结构化的数据格式。 
> (3) 分析数据：可以使用任何方法。 
> (4) 训练算法：此步骤不适用于k-近邻算法。 
> (5) 测试算法：计算错误率。 
> (6) 使用算法：首先需要输入样本数据和结构化的输出结果，然后运行k-近邻算法判定输 入数据分别属于哪个分类，最后应用对计算出的分类执行后续的处理。

## KNN简单实现(1)

>  python=3.7

```python
# Created on 2019/10/24 by wvdon
# website wvdon.com
# 导入使用的包
from numpy import *
import operator
```

创建训练的数据

```python
def createDataSet():
    group = array([[1.0,2.0],[1.2,0.1],[0.1,1.4],[0.3,3.5]])
    labels = ["A","A","B","B"]
    return group,labels
```

利用matplotlib观察数据分布

```python
from matplotlib import pyplot as plt
group,labels = createDataSet()
x = group[:,0]
```

```python
y = group[:,1]
```

```python
plt.scatter(x,y)
plt.show()
```

![png](![在这里插入图片描述](https://img-blog.csdnimg.cn/20210110163017357.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1dlRG9uX3Q=,size_16,color_FFFFFF,t_70#pic_center)

#### 定义一个KNN函数

```python
def classify0(inX,dataSet,labels,k):
    dataSetSize = dataSet.shape[0] #获得训练集的长度
    diffMat = tile(inX,(dataSetSize,1))-dataSet # 复制数组 并将差值计算出来
    #下面三个是计算欧氏距离 
    sqDiffMat = diffMat**2
    sqDistances = sqDiffMat.sum(axis=1)
    distances = sqDistances**0.5 
    
    #获得distances从小到大的索引值
    sortedDistIndicies = distances.argsort()
    classCount = {}
    #找到前K个标签，输出最大的
    for i in range(k):
        voteIlabel = labels[sortedDistIndicies[i]]
        classCount [voteIlabel] = classCount.get(voteIlabel,0)+1
    sortedClassCount = sorted(classCount.items(),key = operator.itemgetter(1),reverse = True)
    return sortedClassCount[0][0]
```

```python
a = classify0([1.2,0.1],group,labels,1)
a
```

```
'A'
```

#### 补充

**欧氏距离**，最常见的两点之间或多点之间的距离表示法，又称之为欧几里得度量，它定义于欧几里得空间中，如点 x = (x1,...,xn),到 y(y1,....yn)

<a href="https://www.codecogs.com/eqnedit.php?latex=dist(X,Y)&space;=&space;\sqrt{(x_1-y_1)^2&plus;(x_2-y_2)^2&plus;····&plus;(x_n-y_n)^2}=\sqrt{\sum_{i=1}^{n}(X_i-Y_i)^2}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?dist(X,Y)&space;=&space;\sqrt{(x_1-y_1)^2&plus;(x_2-y_2)^2&plus;····&plus;(x_n-y_n)^2}=\sqrt{\sum_{i=1}^{n}(X_i-Y_i)^2}" title="dist(X,Y) = \sqrt{(x_1-y_1)^2+(x_2-y_2)^2+····+(x_n-y_n)^2}=\sqrt{\sum_{i=1}^{n}(X_i-Y_i)^2}" /></a>

二位平面上 

<a href="https://www.codecogs.com/eqnedit.php?latex=d&space;=&space;\sqrt{(x_1-y_1)^2&plus;(x_2-y_2)^2}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?d&space;=&space;\sqrt{(x_1-y_1)^2&plus;(x_2-y_2)^2}" title="d = \sqrt{(x_1-y_1)^2+(x_2-y_2)^2}" /></a>

#### numpy 函数

tile  
sum(axis=1) 
argsort()
[numpy总结]()

## 使用 k-近邻算法改进约会网站的配对效果(2)



## 参考

[百度百科]([https://baike.baidu.com/item/k%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95/9512781](https://baike.baidu.com/item/k近邻算法/9512781))