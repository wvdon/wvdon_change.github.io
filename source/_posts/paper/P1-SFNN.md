---
title: 'SFNN: Semantic Features Fusion Neural Network for Multimodal Sentiment Analysis'
date: 2021-1-16 19:52:57
tags: 
	- 
categories: [paper]
description: 用于多模态情感分析的语义特征融合神经网络
---



###  What is this paper doing? 

 sentiment in online reviews  在线情感分析 

### 翻译：

#### 摘要

在线情感分析是一个重要的任务，在线情感分析的影响是这些应用的基础，比如用户偏好模型，消费者行为监测和公共观点分析。在先前的研究中，情感分析任务主要依赖于文本内容，忽视了在评论上视觉信息的模型影响。这篇文章基于提出了一个神经网络SFNN。这个模型第一次使用CNN卷积神经网络和注意力机制去获取图片所表达的情感特征，同时，映射这个情感表达特征到语义特征级别。此时，视觉方面的语义特征与文本模型的语义特征进行联合。最终，评论的情感极性被图片实体层的情感联合特征有效的分析。基于语义级别的特征融合可以减少异构数据的不同。实验结果表明，在基准数据集上，我们的模型比现存的方法能够取得更好的表现。  

### Introduction 介绍

随着互联网的快速发展，越来越多的人进入到网络世界，用户和网站之间的交互会变得越来越频繁，这就导致了大量文本信息的快速增长，据报道，90%的消费者会在买东西之前先读评论，88%的消费者相信评论和熟人的建议。公司为了推荐需要了解用户喜好，或者去跟踪消费者对营销和产品设计的看法。而这一关键就是情感分析。

​	目前，绝大多数的研究者对于永和的情感识别是基于文本的。使用基于情感字典的方法，机器学习规则，基于深度学习的对网民的文本进行情感分析.Kim等人在情感字典的帮助下，通过计算情感得分来判断文本的情感。Pan 等人 第一次使用机器学习算法应用到情感分类任务，使用不同的机器学习模型在电影评论上进行文本分析。Tai等人提出LSTM长短期记忆网络去辨别电影评论的情感，并且取得了好的结果。尽管基于文本的情感识别已经取得了巨大的成就，不过这也是很困难去识别哪些被网民仅仅通过文本，并且他们所表达出来的文本内容是多种含义。

随着多媒体领域的到来，相比之前仅仅通过文本表达情感分方式。网民通常表达他们的情哥哥通过带有图片的文本。网民的情感和图片文本有一个映射关系。综合考虑文本和图片的情感可以一定程度上解决其多义性。通过融合文本和图片情感特征对情感进行分类是一种多模式融合情感识别问题。多模态融合情感识别的核心是对不同模态的特征提取，并且融合提取的特征。目前，多模态融合情感分析的问题尚未引起国内外学者的广泛关注。Rosas使用词袋模型去表示文本特征，通过OpenEar提取音频特征，使用OkaoVision应用于面部表达特征。然后切分不同模型的三个特征向量到长向量，并将它们输入到支持向量机题识别多模态情感。Majumder等人提出了一个新颖的特征层融合方法，这个方法是分层的方式。联合文本特征，语音特征和面部特征。三种特征融合在一起并输入到深度学习模型。 从以上文献可以看出，当前的多模态融合情感识别研究中视觉模态的选择主要是基于面部表情，所采用的方法较为通用，提取图片的语义信息不好。然而，考虑到真正互联网中的舆论事件。面部表情很少出现在网民所发布的帖子中。大多数时候使用的是带有情感极性的图片。

对于以上现存方法的问题，这篇文章提出了一个基于多模态数据的模型SFNN.这篇文章主要的贡献如下：

- 使用图像对模型结构进行排序，图片的语义特征是被转换成语义顺序向量。它相比单独的使用图像特征，可以更好地捕获图像的语义特征。在使用文本特征进行融合后，他可以做更好的情感判断。
- 使用BERT预训练模型进行提取评论文本特征，他相比传统的Text-CNN，LSTM,BiGRU 能够更好的捕获句子之间的关系。

这篇文章的结构如下。第二段介绍了在相关领域其他学者的工作。在第三段对于模型介绍和推导。第四段和第五段分别给出实验结果，实验结论，未来工作的展望。

### 相关工作

在线评论情感识别涉及到识别网民在社交平台发布的文本的情感。网民情感是在线评论的一个重要的元素。对于识别也有重大的实际意义。**鉴于没有针对文本数据进行感性标记的统一标准，**Wu等人基于心理模型OCC构造了文本情感标注规则，结合CNNs模型在深度学习中识别网民的情感，取得了良好的效果。He等人利用词向量表示技术，针对微博中表情符号的特征，构建了表情符号情感空间的特征表示矩阵，形成了具有情感极性的情感词向量。Wu等人从网民在突发事件中的真实情绪出发，提出了将情感词向量与BiLSTM相结合的模型，对网民的负面情绪进行识别，并将其分为愤怒、悲伤和恐惧三类。这些方法只关注评论的文本信息进行情感识别，缺乏对评论中的图像信息的关注。

​	图像情感识别是一个新的研究领域。在图像情感识别任务中，研究者们做出了不懈的努力。图像情感识别技术主要可分为三类:基于低级视觉特征的方法,通过提取出低级视觉特征与人类情绪有关,如颜色和其他手工特性,结合相关知识,使用分类器分类图片的情绪。基于中间层语义表示的方法是通过构建中层语义特征来弥补底层特征与人的情感之间的语义鸿沟，形容词和名词对是中层语义表达的一种典型方法。基于深度学习的方法，利用深度神经网络构建深度模型，对图片的情感进行分类。深度学习在计算机视觉领域取得了重大突破。You等在迁移学习的基础上，在ImageNet上使用了预先训练好的CNNs模型，对300多万张不同情感的弱标记图像进行了微调，结果表明CNNs模型与机器学习相比具有很大的优势。

​	总的来说，现有的关于网络舆情中互联网用户情感识别的研究大多是基于文本，利用自然处理技术对文本情感进行识别。然而，随着多媒体时代的到来，网民情绪的表达方式发生了变化。与以往仅通过文字表达情感的方式相比，互联网用户更倾向于通过文字与图片相结合的方式来表达情感。为此，本文提出了一种基于多模态数据的情感分析模型。首先**利用注意机制提取图像中的重要语义特征，然后利用递归神经网络将语义特征转化为不带标注的特殊图像语义向量。最后，利用注意机制对特征进行权重赋值，并利用图像语义向量对Bert提取的文本语义向量进行增强**。实验表明，本文提出的模型在网络评论情感分析方面比现有模型有更好的效果。

### 模型概述

#### 模型介绍

​	用户评论通常包含文本和视觉内容，两者都提供重要的补充信息。因此，在网络评论的情感识别中，多模态检测方法比单模态检测方法具有更好的泛化性能。考虑到传统模型不能很好地提取图像部分的语义信息，我们提出了SFNN模型，如图1所示。

​	本文提出的模型集成了四个主要模块:视觉特征提取模块、文本特征提取模块、视觉语义向量提取模块和多模态特征融合模块。下面将详细介绍其模块的具体推导过程。

#### 视觉特征提取模块

为了获得视觉部分良好的语义表示，我们首先使用VGG预训练模型对输入图像进行编码。VGG卷积神经网络是对许多图像相关任务的图像表示学习有效。我们使用VGG16来获得图像的表示，通过模型输入得到最后一个全连接层的输出，然后再进入分类层。图像表示是一个从图像编码的4096维向量。

然后我们利用注意机制来接受图像的语义特征，目的是突出图像中具有强烈情感表达的部分，并且某些部分对情感分析更有意义。因此，当获得一个视觉模态表示时，每个特征将被赋予一个权值，表示其在视觉模态表示中的“重要性”。我们使用柔和的注意力。

#### 图像语义特征提取模块

在视觉特征提取模块中，我们得到了图像的特征表达。为了获得更好的图像语义表达，我们使用一个BiGRU来形成图像序列模块。图像到序列模块通常用于图像标题，它用于将图像特征与文本特征对齐。我们将图像的特征表达式发送给BiGRU，得到图像的语义向量，这有助于表示图像的语义信息，如式(5)所示:

#### 文本特征提取模块

传统的词向量模型对短句和无歧义句的模态分析有较好的效果。然而，在现实中处理过的句子并没有那么简单。要解决一词多义的问题，就要充分考虑词的前后关系。BERT模型是谷歌提出的一种新的语言表示模型。与传统的文本情感分析模型相比，该模型能够更好地覆盖上下文之间的联系。本文使用谷歌Research发布的预训练模型BERTBase来提取文本特征。对于输入文本，我们使用Bert进行特征提取，如式(6)所示:

为了更好地捕捉全局特征信息，更好地与图像语义信息融合，我们在提取BERT后，使用BiGRU进行特征提取，如式(7)所示:

#### 多模态特征融合模块

对于提取的图像特征和文本特征，我们提出了一种融合网络来拼接这些特征。具体来说，我们首先融合视觉模态的语义向量和文本模态h = [hv 1, hv 2，…]，hv i, ht 1, ht 2，…，然后将拼接好的特征发送到注意机制中。我们通过注意机制来突出那些有价值的特征，计算增强后的特征表示法，如式(8)~(10)所示:

然后我们将使用主成分分析(PCA)算法来融合特征e=[e1, e2, e3，…]， en]T以图像的原始特征i=[i1, i2, i3，…]，在]T中，PCA可以减少不同数据的差异。通过PCA算法，我们可以得到最终的特征向量f。最后，使用Softmax对最终融合的特征进行分类，如式(11)所示:

在本节中，我们将描述用于实验的数据集，并报告结果和必要的分析

### 实验

#### 数据集

我们使用了来自Yelp.com的在线食物和餐馆评论数据集，覆盖了美国的五个主要城市:波士顿(BO)、芝加哥(CH)、洛杉矶(LA)、纽约(NY)和旧金山(SF)。洛杉矶是最大的，有最多的文件和图像。波士顿(BO)是最小的。然而，就句子数量和单词数量而言，这五个城市的文件长度非常相似。该数据集共有超过44,000条评论，其中包括244,000张图片。对于我们的实验，每个合成至少有3张图像。

#### 基线

在本节中，我们比较了SFNN模型与之前提出的多模态情感分析模型的性能。我们比较以下基线:

- BiGRU - avgg和BiGRU - mvgg是连接BiGRU从文本学习的表示和VGG从图像学习的表示的组合，并将它们提供给一个分类层。

- HAN-aVGG和HAN-mVGG是文字类的HAN-ATT(文字情感分析前沿)和图像类的VGG的结合。hanatt[17]使用单词和句子编码器来实现文档的层次结构。
- VistaNet[18]提出,照片不独立表达文本的情绪,而是突出显示的文本的一个辅助部分的内容,它使用图片指导文本的关注,这决定了文档中的重要性程度不同的句子情感分类的文档。

#### 结果

从表I可以看出，与其他模型相比，我们的SFNN模型在Yelp上获得了最好的结果62.80%，比VistaNet模型提高了2.1%

​	TFN模型和HAN模型提供了丰富的文本特征和视觉特征之间的交互，但对数据的性能较差，在比较方法中准确率最低，分别为43.89%、46.87%、53.16%和55.01%。原因在于分析的直接融合功能的表达图片和文本的功能表达,和图像的语义信息不能得到,因为这些特性不携带相同的情绪推动信息,和他们保持更多的物理层的视觉模式。Truong提出的VistaNet模型并没有直接利用图像的特征，而是将图像转化为注意权重来突出文本中的情感信息。可见，VistaNet对数据集的准确率为61.88%。虽然我们提出的模型也文本特征的融合和图像特性,与之前的方法相比,我们的模型可以更好地提取图像语义信息,融合的特性在语义层面,并协助与图像的特性在物理层面,我们得到更好的实验结果。

#### 架构消融分析

为了研究SFNN的每个组件的贡献，我们进行了消融分析，从最基本的配置开始，增加构建完整架构的组件。结果汇总于表二。

​	我们从文本特征提取模块开始，只依赖文本。从第一行可以看出，平均准确率达到58.84%。然后我们将文本特征与VGG16提取的图像特征直接拼接，可以看到效果提高了2.6%。在加入图像语义提取模块后，我们得到图像和文本在语义层面的特征。两者直接融合后，我们的平均识别准确率比之前提高了5.6%。最后，加入多模态特征融合模块，利用注意机制和主成分分析方法(PCA)对图像的语义级特征进行融合，并分别对图像的文字和物理层面特征进行了分析。最终，平均精度达到62.80%。实验结果表明，该模型的每个组成部分都有自己的贡献。

### 结论

本文研究了多模态评论情感分析问题。针对现有模型大多直接拼接图像特征和文本特征，而不能获得图像的语义特征的问题，我们提出了一种新的多模态评论情感分析模型SFNN。具体来说，我们的模型集成了四个主要组件:视觉特征提取模块、文本特征提取模块、视觉语义向量提取模块和多模态特征融合模块。通过这四个模块的协同工作，我们可以在语义层面上融合图像语义向量和文本语义向量，得到更好的情感表达，同时结合图像的物理特征对评论进行分析。在美国5个主要城市的实验数据表明，我们的SFNN模型在情感分析方面优于多模态基线。该模型比现有模型具有更好的鲁棒性。