---
title: '文本特征处理'
date: 2020-02-27 15:50:39
tags: 
  - nlp
  - 特征处理
categories: [machineLearning]
mathjax: true
---

# 文本特征提取

[参考](https://www.cnblogs.com/ljhdo/p/10578047.html)

机器学习算法往往无法直接处理文本数据，需要把文本数据转换为数值型数据

### One-Hot

One-Hot方法很简单，但是它的问题也很明显：

- 没有考虑单词之间的相对位置，任意两个词之间都是孤立的；
- 如果文档中有很多词，词向量会有很多列，但是只有一个列的值是1；

**One-Hot表示的应用**

sklearn使用词袋（Bag of Words）和TF-IDF模型来表示文本数据，这两个模型都是One-Hot表示的应用，其中，词袋模型对应的就是文档向量。

### 词袋模型

词袋模型（BoW）是用于文本表示的最简单的方法， BoW把文本转换为文档中单词出现次数的矩阵，该模型只关注文档中是否出现给定的单词和单词出现频率，而舍弃文本的结构、单词出现的顺序和位置。

```python
CountVectorizer(input=’content’, encoding=’utf-8’, decode_error=’strict’, strip_accents=None, 
                lowercase=True,    preprocessor=None, tokenizer=None, stop_words=None, 
                token_pattern=’(?u)\b\w\w+\b’, ngram_range=(1, 1), analyzer=’word’, 
                max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, 
                dtype=<class ‘numpy.int64’>)
```

常用参数注释:

- input：默认值是content，表示输入的是顺序的字符文本
- decode_error：默认为strict，遇到不能解码的字符将报UnicodeDecodeError错误，设为ignore将会忽略解码错误
- lowercase：默认值是True，在分词（Tokenize）之前把文本中的所有字符转换为小写。
- preprocessor：预处理器，在分词之前对文本进行预处理，默认值是None
- tokenizer：分词器，把文本字符串拆分成各个单词（token），默认值是None
- analyzer：用于预处理和分词，可设置为string类型，如’word’, ‘char’, ‘char_wb’，默认值是word
- stop_words：停用词表，如果值是english，使用内置的英语停用词列表；如果是一个列表，那么使用该列表作为停用词，设为None且max_df∈[0.7, 1.0)将自动根据当前的语料库建立停用词表
- ngram_range：tuple（min_n，max_n），表示ngram模型的范围
- max_df：可以设置为范围在[0.0 1.0]的浮点数，也可以设置为没有范围限制的整数，默认为1.0。这个参数的作用是作为一个阈值，当构造语料库的词汇表时，如果某个词的document frequence大于max_df，这个词不会被当作关键词。如果这个参数是float，则表示词出现的次数与语料库文档数的百分比，如果是int，则表示词出现的次数。如果参数中已经给定了vocabulary，则这个参数无效
- min_df：类似于max_df，不同之处在于如果某个词的document frequence小于min_df，则这个词不会被当作关键词
- max_features：对所有关键词的term frequency进行降序排序，只取前max_features个作为关键词集
- vocabulary：默认为None，自动从输入文档中构建关键词集，也可以是一个字典或可迭代对象。
- binary：默认为False，一个关键词在一篇文档中可能出现n次；如果binary=True，非零的n将全部置为1，这对需要布尔值输入的离散概率模型的有用的
- **dtype** ：用于设置fit_transform() 或 transform()函数返回的矩阵元素的数据类型

模型的属性和方法：

- vocabulary_：词汇表，字典类型
- get_feature_names()：所有文本的词汇，列表型
- stop_words_：停用词列表

模型的主要方法：

- fit(raw_document)：拟合模型，对文本分词，并构建词汇表等
- transform(raw_documents)：把文档转换为文档-词矩阵
- fit_transform(raw_documents)：拟合文档，并返回该文档的文档-词矩阵

### TF-IDF

TF-IDF模型用于对特征信息量进行缩放，当一个词在特定的文档中经常出现，而在其他文档中出现的频次很低，那么给予该词较高的权重；当一次词在多个文档中出现的频次都很高，那么给予该词较低的权重。如果一次单词在特定的文档中出现的频次很高，而在其他文档中出现的频次很低，那么这个单词很可能是该文档独有的词，能够很好地描述该文档。

**1，TF-IDF模型计算原理**

TF（ Term Frequency）是词频，表示每个单词在文档中的数量（频数），TF依赖于BoW模型的输出。

IDF（Inverse Document Frequency）是逆文档频率，代表一个单词的普遍成都，当一个词越普遍（即有大量文档包含这个词）时，其IDF值越低；反之，则IDF值越高。IDF是包含该单词的文档数量和文档总数的对数缩放比例

![img](https://img2018.cnblogs.com/blog/628084/201903/628084-20190322180339455-653606013.png)

TF-IDF（术语频率 - 逆文档频率）模型是TF和IDF相乘的结果：TF-IDF=TF*IDF。

```python
TfidfVectorizer(input=’content’, encoding=’utf-8’, decode_error=’strict’, strip_accents=None, 
                lowercase=True,    preprocessor=None, tokenizer=None, stop_words=None, 
                token_pattern=’(?u)\b\w\w+\b’, ngram_range=(1, 1), analyzer=’word’, 
                max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, 
                dtype=dtype=<class ‘numpy.float64’>, 
                norm=’l2’, use_idf=True, smooth_idf=True, sublinear_tf=False)
```

大部分参数和CountVectorizer相同，TfidfVectorizer独有的参数注释：

- norm=’l2’：每个输出行具备单位规范，当引用'l2'范式时，所有向量元素的平方和为1；当应用l2范数时，两个向量之间的余弦相似度是它们的点积。 *'l1'：向量元素的绝对值之和为1。
- use_idf=True：启用IDF来重新加权
- smooth_idf=True：平滑idf权重，向文档-词频矩阵的所有位置加1，就像存在一个额外的文档，只包含词汇表中的每个术语一次，目的是为了防止零分裂。
- sublinear_tf=False：应用次线性tf缩放，默认值是False